{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 네트워크를 사용해서 텍스트 생성하기 (라이브러리 없이)\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데모**<br>\n",
    "\n",
    "Eminem 가사 생성을 위해 numpy만 사용해서 구축한 LSTM 네트워크를 트레이닝 시킬것이다. LSTM은 뉴럴 네트워크의 심플한 확장 버전이고, 지난 몇 년간 딥러닝의 많은 성취에 기여했다.<br><br>\n",
    "\n",
    "*eminem의 lose yourself를 임의로 가져다가 txt 파일로 만들었습니다. 코드는 돌아갈 정도로만 수정했습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Network는 무엇인가?**<br>\n",
    "\n",
    "\n",
    "Recurrent nets은 데이터의 시퀀스를 배우는데에 유용하다. Input, Hidden state, output<br><br>\n",
    "\n",
    "<img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png' width='200' height='200'><br><br>\n",
    "\n",
    "\n",
    "input에서 hidden state로 이어진 weight matrix다. weight matrix는 hidden state에서 이전 time step의 hidden state로도 연결되어 있다.<br><br>\n",
    "<img src='https://iamtrask.github.io/img/basic_recurrence_singleton.png' width='300' height='300'><br><br>\n",
    "\n",
    "\n",
    "그래서 다음 training iteration의 input뿐 아니라 input + 이전의 hidden state를 전달하는, 시간마다 자기자신과 연결되는 feedforward network로 간주할 수도 있다.<br><br>\n",
    "<img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png' width='500' height='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>**Recurrent Network의 문제점**<br><br>\n",
    "\n",
    "만약 우리가 \"The grass is green\" 문장의 마지막 단어를 예측하려고 한다면? 가능하다.<br>\n",
    "<img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png' width='300' height='300'><br><br><br>\n",
    "\n",
    "만약 우리가 \"I am French (2000 단어 뒤에) i speak fluent French\" 문장 뒤의 마지막 단어를 예측하려고 한다면? dependencies 길게 기억하는 일이 필요한데, RNN은 이걸 잘 못한다.<br>\n",
    "<img src='http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png' width='400' height='400'><br><br><br>\n",
    "\n",
    "이건 \"Vanishing Gradient Problem\"이라고 하는데, backpropagation 때 Gradient가 exponentially하게 줄어드는 현상이다.<br><br>\n",
    "<img src='http://slideplayer.com/slide/5251503/16/images/6/Recurrent+Neural+Networks.jpg' width='300' height='300'><br>\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*8JJ6sYleUtvUZR7TOyyFVg.png' width='300' height='300'><br><br><br>\n",
    "\n",
    "gradient의 magnitude에 영향을 끼치는 두 개의 factor가 있다 - gradient를 지나가는 weight와 activation function이다. (더 정확하게는, 이 녀석들의 도함수-derivatives)<br>\n",
    "만약 이 factor중 하나가 1보다 작다면, gradient는 사라질수도 있다; 만약 1보다 크다면, 폭발할 수 있다. 해결책은 LSTM 셀을 넣으면 된다!<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM 셀 (Long-Short Term Memory Cell)**<br><br>\n",
    "\n",
    "우리 모델 업데이트를 어떻게 할지 어떤 제약도 없다. 그러니 knowledge를 좀 무질서하게 바꿀수도 있다: <br>\n",
    "한 프레임에는 미국에 있는 characters, 다음 프레임에는 일본에 있다고 생각되어지는, 초밥을 먹는 characters를 본다. \n",
    "<br>그리고 또 다음 프레임은 하이드라섬에 있다고 생각되어지는, 북극곰을 본다.<br><br>\n",
    "\n",
    "이 혼란의 의미는 정보는 빠르게 사라지고 바뀐다는 말이다. 모델이 long-term memory를 지키기 어렵다는 말.<br>\n",
    "그러니 네트워크가 배우게 하려면 어떻게 its beliefs를 업데이트 해야하는지를 배우게 하는게 좋다. <br>(Bob이 없는 씬은 Bob-related information을 바꿔선 안되고, Alice씬은 그녀에 대한 세부 정보들을 모으는데에 중점을 둬야한다.)<br><br>\n",
    "\n",
    "이건 일반적인 RNN 셀을 input, forget, output gate를 써서 대체한다. cell state가 그러하듯.<br><br>\n",
    "<img src='https://www.researchgate.net/profile/Mohsen_Fayyaz/publication/306377072/figure/fig2/AS:398082849165314@1471921755580/Fig-2-An-example-of-a-basic-LSTM-cell-left-and-a-basic-RNN-cell-right-Figure.ppm' width='400' height='400'><br>\n",
    "<img src='https://kijungyoon.github.io/assets/images/lstm.png' width='300' height='300'><br><br>\n",
    "\n",
    "\n",
    "이 gates들은 각자의 weight values의 set을 갖고 있다. 전부 미분가능해야 하는데 그래야 우리가 backprop을 할 수 있다.<br>(우리가 얘네들을 사용해서 gradient를 계산하고 weights를 업데이트한다는 의미다)<br> 우리의 모델이 무엇을 forget해야하는지, 무엇을 remember해야 하는지 알았으면 좋겠다.<br> 그러니 새로운 것이 들어올 때, 모델은 더 이상 필요없다 판단한 어떤 long-term information을 첫번째로 잊는다. <br>그리고 새로운 input의 일부 중  사용할 가치가 있는 것을 배운다, 그리고 long-term memory에 저장한다.<br><br>\n",
    "\n",
    "항상 full long-term memory를 사용하는 대신, 어떤 부분에 집중해야 하는지를 배운다.<br>\n",
    "기본적으로, 우리는 forgetting, remembering, 그리고 attention을 위한 mechanisms이 필요하다. LSTM이 이 역할을 한다.<br><br>\n",
    "\n",
    "vanilla RNN이 hidden state/memory 업데이트를 위해 하나의 equation을 사용하는 것에 비해:<br><br><br>\n",
    "<img src='http://i.imgur.com/nT4VBPf.png' width='700' height='500'><br><br>\n",
    "\n",
    "long term memory의 어떤 부분을 기억해야 하고 잊어야 할까?<br>\n",
    "우리는 remember gate를 배우기 위해 새로운 input과 working memory를 사용할거다.<br>\n",
    "new data의 어떤 부분을 사용해야 하고 저장해야 할까?<br>\n",
    "attention vector를 사용해서 working memory를 업데이트 한다.<br><br>\n",
    "\n",
    "- long-term memory는 대개 cell state로 불리운다.<br>\n",
    "- working memory는 대개 hidden state로 불리운다. vanilla RNN의 hidden state와 유사하다.<br>\n",
    "- remember vector는 대개 forget gate(forget gate의 1은 여전히 keep the memory를 의미하고, 0은 여전히 forget it을 의미함에도)로 불리운다.<br>\n",
    "- save vector는 대개 input gate(얼마만큼의 input을 cell state에 넣을지를 결정한다.)로 불리운다.<br>\n",
    "- focus vector는 대개 output gate로 불리운다<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**활용 케이스**<br>\n",
    "\n",
    "비디오<br>\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/mLxsbWAYIpw/0.jpg)](https://www.youtube.com/watch?time_continue=14&v=mLxsbWAYIpw)<br><br>\n",
    "\n",
    "현재로서 가장 유명한 application은 단어들이나 문장들, sound spectrogram 등등 sequential data와 관련있는 Natural Language Processing이다. <br>번역이나 sentiment analysis, 텍스트 생성 등등 에도 쓰인다.<br>\n",
    "\n",
    "lstm의 application으로 덜 명확한 분야로는 이미지 분류(각 사진의 pixel을 행 by 행으로 feeding한다)나 딥마인드의 deep Q learning agents가 있다. <br><br>\n",
    "\n",
    "**다른 끝내주는 예제들**<br>\n",
    "\n",
    "텐서플로우로 음성 인식 (Speech recognition Tensorflow) - https://github.com/zzw922cn/Automatic_Speech_Recognition <br>\n",
    "LSTM 비주얼라이제이션 (LSTM visualization) - https://github.com/HendrikStrobelt/LSTMVis<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단계들**<br><br>\n",
    "\n",
    "1. RNN 클래스 구축<br>\n",
    "2. LSTM cell 클래스\n",
    "구축<br>\n",
    "3. 데이터 로딩 함수<br>\n",
    "3. 트레이닝 시간!<br><br>\n",
    "\n",
    "<img src='http://eric-yuan.me/wp-content/uploads/2015/06/5.jpg' width='300' height='300'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    #input (word), expected output (next word), num of words (num of recurrences), array expected outputs, learning rate\n",
    "    def __init__ (self, xs, ys, rl, eo, lr):\n",
    "        #initial input (first word)\n",
    "        self.x = np.zeros(xs)\n",
    "        #input size \n",
    "        self.xs = xs\n",
    "        #expected output (next word)\n",
    "        self.y = np.zeros(ys)\n",
    "        #output size\n",
    "        self.ys = ys\n",
    "        #weight matrix for interpreting results from LSTM cell (num words x num words matrix)\n",
    "        self.w = np.random.random((ys, ys))\n",
    "        #matrix used in RMSprop\n",
    "        self.G = np.zeros_like(self.w)\n",
    "        #length of the recurrent network - number of recurrences i.e num of words\n",
    "        self.rl = rl\n",
    "        #learning rate \n",
    "        self.lr = lr\n",
    "        #array for storing inputs\n",
    "        self.ia = np.zeros((rl+1,xs))\n",
    "        #array for storing cell states\n",
    "        self.ca = np.zeros((rl+1,ys))\n",
    "        #array for storing outputs\n",
    "        self.oa = np.zeros((rl+1,ys))\n",
    "        #array for storing hidden states\n",
    "        self.ha = np.zeros((rl+1,ys))\n",
    "        #forget gate \n",
    "        self.af = np.zeros((rl+1,ys))\n",
    "        #input gate\n",
    "        self.ai = np.zeros((rl+1,ys))\n",
    "        #cell state\n",
    "        self.ac = np.zeros((rl+1,ys))\n",
    "        #output gate\n",
    "        self.ao = np.zeros((rl+1,ys))\n",
    "        #array of expected output values\n",
    "        self.eo = np.vstack((np.zeros(eo.shape[0]), eo.T))\n",
    "        #declare LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "        self.LSTM = LSTM(xs, ys, rl, lr)\n",
    "    \n",
    "    #activation function. simple nonlinearity, convert nums into probabilities between 0 and 1\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #the derivative of the sigmoid function. used to compute gradients for backpropagation\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))    \n",
    "    \n",
    "    #lets apply a series of matrix operations to our input (curr word) to compute a predicted output (next word)\n",
    "    def forwardProp(self):\n",
    "        for i in range(1, self.rl+1):\n",
    "#             print(i,'아이임당')\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            #store computed cell state\n",
    "            self.ca[i] = cs\n",
    "            self.ha[i] = hs\n",
    "            self.af[i] = f\n",
    "            self.ai[i] = inp\n",
    "            self.ac[i] = c\n",
    "            self.ao[i] = o\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            self.x = self.eo[i-1]\n",
    "        return self.oa\n",
    "   \n",
    "    \n",
    "    def backProp(self):\n",
    "        #update our weight matrices (Both in our Recurrent network, as well as the weight matrices inside LSTM cell)\n",
    "        #init an empty error value \n",
    "        totalError = 0\n",
    "        #initialize matrices for gradient updates\n",
    "        #First, these are RNN level gradients\n",
    "        #cell state\n",
    "        dfcs = np.zeros(self.ys)\n",
    "        #hidden state,\n",
    "        dfhs = np.zeros(self.ys)\n",
    "        #weight matrix\n",
    "        tu = np.zeros((self.ys,self.ys))\n",
    "        #Next, these are LSTM level gradients\n",
    "        #forget gate\n",
    "        tfu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #input gate\n",
    "        tiu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #cell unit\n",
    "        tcu = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #output gate\n",
    "        tou = np.zeros((self.ys, self.xs+self.ys))\n",
    "        #loop backwards through recurrences\n",
    "        for i in range(self.rl, -1, -1):\n",
    "            #error = calculatedOutput - expectedOutput\n",
    "            error = self.oa[i] - self.eo[i]\n",
    "            #calculate update for weight matrix\n",
    "            #(error * derivative of the output) * hidden state\n",
    "            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n",
    "            #Time to propagate error back to exit of LSTM cell\n",
    "            #1. error * RNN weight matrix\n",
    "            error = np.dot(error, self.w)\n",
    "            #2. set input values of LSTM cell for recurrence i (horizontal stack of arrays, hidden + input)\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n",
    "            #3. set cell state of LSTM cell for recurrence i (pre-updates)\n",
    "            self.LSTM.cs = self.ca[i]\n",
    "            #Finally, call the LSTM cell's backprop, retreive gradient updates\n",
    "            #gradient updates for forget, input, cell unit, and output gates + cell states & hiddens states\n",
    "            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n",
    "            #calculate total error (not necesarry, used to measure training progress)\n",
    "            totalError += np.sum(error)\n",
    "            #accumulate all gradient updates\n",
    "            #forget gate\n",
    "            tfu += fu\n",
    "            #input gate\n",
    "            tiu += iu\n",
    "            #cell state\n",
    "            tcu += cu\n",
    "            #output gate\n",
    "            tou += ou\n",
    "        #update LSTM matrices with average of accumulated gradient updates    \n",
    "        self.LSTM.update(tfu/self.rl, tiu/self.rl, tcu/self.rl, tou/self.rl) \n",
    "        #update weight matrix with average of accumulated gradient updates  \n",
    "        self.update(tu/self.rl)\n",
    "        #return total error of this iteration\n",
    "        return totalError\n",
    "    \n",
    "    def update(self, u):\n",
    "        #vanilla implementation of RMSprop\n",
    "        self.G = 0.9 * self.G + 0.1 * u**2  \n",
    "        self.w -= self.lr/np.sqrt(self.G + 1e-8) * u\n",
    "        return\n",
    "    \n",
    "    #this is where we generate some sample text after having fully trained our model\n",
    "    #i.e error is below some threshold\n",
    "    def sample(self):\n",
    "        #loop through recurrences - start at 1 so the 0th entry of all arrays will be an array of 0's\n",
    "        for i in range(1, self.rl+1):\n",
    "            #set input for LSTM cell, combination of input (previous output) and previous hidden state\n",
    "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
    "            #run forward prop on the LSTM cell, retrieve cell state and hidden state\n",
    "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
    "            #store input as vector\n",
    "            maxI = np.argmax(self.x)\n",
    "            self.x = np.zeros_like(self.x)\n",
    "            self.x[maxI] = 1\n",
    "            self.ia[i] = self.x #Use np.argmax?\n",
    "            #store cell states\n",
    "            self.ca[i] = cs\n",
    "            #store hidden state\n",
    "            self.ha[i] = hs\n",
    "            #forget gate\n",
    "            self.af[i] = f\n",
    "            #input gate\n",
    "            self.ai[i] = inp\n",
    "            #cell state\n",
    "            self.ac[i] = c\n",
    "            #output gate\n",
    "            self.ao[i] = o\n",
    "            #calculate output by multiplying hidden state with weight matrix\n",
    "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
    "            #compute new input\n",
    "            maxI = np.argmax(self.oa[i])\n",
    "            newX = np.zeros_like(self.x)\n",
    "            newX[maxI] = 1\n",
    "            self.x = newX\n",
    "        #return all outputs    \n",
    "        return self.oa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='http://i.imgur.com/BUAVEZg.png' width='300' height='300'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    # LSTM cell (input, output, amount of recurrence, learning rate)\n",
    "    def __init__ (self, xs, ys, rl, lr):\n",
    "        #input is word length x word length\n",
    "        self.x = np.zeros(xs+ys)\n",
    "        #input size is word length + word length\n",
    "        self.xs = xs + ys\n",
    "        #output \n",
    "        self.y = np.zeros(ys)\n",
    "        #output size\n",
    "        self.ys = ys\n",
    "        #cell state intialized as size of prediction\n",
    "        self.cs = np.zeros(ys)\n",
    "        #how often to perform recurrence\n",
    "        self.rl = rl\n",
    "        #balance the rate of training (learning rate)\n",
    "        self.lr = lr\n",
    "        #init weight matrices for our gates\n",
    "        #forget gate\n",
    "        self.f = np.random.random((ys, xs+ys))\n",
    "        #input gate\n",
    "        self.i = np.random.random((ys, xs+ys))\n",
    "        #cell state\n",
    "        self.c = np.random.random((ys, xs+ys))\n",
    "        #output gate\n",
    "        self.o = np.random.random((ys, xs+ys))\n",
    "        #forget gate gradient\n",
    "        self.Gf = np.zeros_like(self.f)\n",
    "        #input gate gradient\n",
    "        self.Gi = np.zeros_like(self.i)\n",
    "        #cell state gradient\n",
    "        self.Gc = np.zeros_like(self.c)\n",
    "        #output gate gradient\n",
    "        self.Go = np.zeros_like(self.o)\n",
    "    \n",
    "    #activation function to activate our forward prop, just like in any type of neural network\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    #derivative of sigmoid to help computes gradients\n",
    "    def dsigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    #tanh! another activation function, often used in LSTM cells\n",
    "    #Having stronger gradients: since data is centered around 0, \n",
    "    #the derivatives are higher. To see this, calculate the derivative \n",
    "    #of the tanh function and notice that input values are in the range [0,1].\n",
    "    def tangent(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    #derivative for computing gradients\n",
    "    def dtangent(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    #lets compute a series of matrix multiplications to convert our input into our output\n",
    "    def forwardProp(self):\n",
    "        f = self.sigmoid(np.dot(self.f, self.x))\n",
    "        self.cs *= f\n",
    "        i = self.sigmoid(np.dot(self.i, self.x))\n",
    "        c = self.tangent(np.dot(self.c, self.x))\n",
    "        self.cs += i * c\n",
    "        o = self.sigmoid(np.dot(self.o, self.x))\n",
    "        self.y = o * self.tangent(self.cs)\n",
    "        return self.cs, self.y, f, i, c, o\n",
    "    \n",
    "   \n",
    "    def backProp(self, e, pcs, f, i, c, o, dfcs, dfhs):\n",
    "        #error = error + hidden state derivative. clip the value between -6 and 6.\n",
    "        e = np.clip(e + dfhs, -6, 6)\n",
    "        #multiply error by activated cell state to compute output derivative\n",
    "        do = self.tangent(self.cs) * e\n",
    "        #output update = (output deriv * activated output) * input\n",
    "        ou = np.dot(np.atleast_2d(do * self.dtangent(o)).T, np.atleast_2d(self.x))\n",
    "        #derivative of cell state = error * output * deriv of cell state + deriv cell\n",
    "        dcs = np.clip(e * o * self.dtangent(self.cs) + dfcs, -6, 6)\n",
    "        #deriv of cell = deriv cell state * input\n",
    "        dc = dcs * i\n",
    "        #cell update = deriv cell * activated cell * input\n",
    "        cu = np.dot(np.atleast_2d(dc * self.dtangent(c)).T, np.atleast_2d(self.x))\n",
    "        #deriv of input = deriv cell state * cell\n",
    "        di = dcs * c\n",
    "        #input update = (deriv input * activated input) * input\n",
    "        iu = np.dot(np.atleast_2d(di * self.dsigmoid(i)).T, np.atleast_2d(self.x))\n",
    "        #deriv forget = deriv cell state * all cell states\n",
    "        df = dcs * pcs\n",
    "        #forget update = (deriv forget * deriv forget) * input\n",
    "        fu = np.dot(np.atleast_2d(df * self.dsigmoid(f)).T, np.atleast_2d(self.x))\n",
    "        #deriv cell state = deriv cell state * forget\n",
    "        dpcs = dcs * f\n",
    "        #deriv hidden state = (deriv cell * cell) * output + deriv output * output * output deriv input * input * output + deriv forget\n",
    "        #* forget * output\n",
    "        dphs = np.dot(dc, self.c)[:self.ys] + np.dot(do, self.o)[:self.ys] + np.dot(di, self.i)[:self.ys] + np.dot(df, self.f)[:self.ys] \n",
    "        #return update gradinets for forget, input, cell, output, cell state, hidden state\n",
    "        return fu, iu, cu, ou, dpcs, dphs\n",
    "            \n",
    "    def update(self, fu, iu, cu, ou):\n",
    "        #update forget, input, cell, and output gradients\n",
    "        self.Gf = 0.9 * self.Gf + 0.1 * fu**2 \n",
    "        self.Gi = 0.9 * self.Gi + 0.1 * iu**2   \n",
    "        self.Gc = 0.9 * self.Gc + 0.1 * cu**2   \n",
    "        self.Go = 0.9 * self.Go + 0.1 * ou**2   \n",
    "        \n",
    "        #update our gates using our gradients\n",
    "        self.f -= self.lr/np.sqrt(self.Gf + 1e-8) * fu\n",
    "        self.i -= self.lr/np.sqrt(self.Gi + 1e-8) * iu\n",
    "        self.c -= self.lr/np.sqrt(self.Gc + 1e-8) * cu\n",
    "        self.o -= self.lr/np.sqrt(self.Go + 1e-8) * ou\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LoadText():\n",
    "    #open text and return input and output data (series of words)\n",
    "    with open(\"eminem_lose_yourself.txt\", \"r\") as text_file:\n",
    "        data = text_file.read()\n",
    "    text = list(data)\n",
    "    outputSize = len(text)\n",
    "    data = list(set(text))\n",
    "    uniqueWords, dataSize = len(data), len(data) \n",
    "    returnData = np.zeros((uniqueWords, dataSize))\n",
    "    for i in range(0, dataSize):\n",
    "        returnData[i][i] = 1\n",
    "    returnData = np.append(returnData, np.atleast_2d(data), axis=0)\n",
    "    output = np.zeros((uniqueWords, outputSize))\n",
    "    for i in range(0, outputSize):\n",
    "        index = np.where(np.asarray(data) == text[i])\n",
    "        output[:,i] = returnData[0:-1,index[0]].astype(float).ravel()  \n",
    "    return returnData, uniqueWords, output, outputSize, data\n",
    "\n",
    "#write the predicted output (series of words) to disk\n",
    "def ExportText(output, data):\n",
    "    finalOutput = np.zeros_like(output)\n",
    "    prob = np.zeros_like(output[0])\n",
    "    outputText = \"\"\n",
    "    print(len(data))\n",
    "    print(output.shape[0])\n",
    "    for i in range(0, output.shape[0]):\n",
    "        for j in range(0, output.shape[1]):\n",
    "            prob[j] = output[i][j] / np.sum(output[i])\n",
    "        outputText += np.random.choice(data, p=prob)    \n",
    "    with open(\"output.txt\", \"w\") as text_file:\n",
    "        text_file.write(outputText)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Begin program    \n",
    "print(\"Beginning\")\n",
    "iterations = 5000\n",
    "learningRate = 0.001\n",
    "#load input output data (words)\n",
    "returnData, numCategories, expectedOutput, outputSize, data = LoadText()\n",
    "print(\"Done Reading\")\n",
    "#init our RNN using our hyperparams and dataset\n",
    "RNN = RecurrentNeuralNetwork(numCategories, numCategories, outputSize, expectedOutput, learningRate)\n",
    "\n",
    "#training time!\n",
    "for i in range(1, iterations, 100):\n",
    "    #compute predicted next word\n",
    "    RNN.forwardProp()\n",
    "    #update all our weights using our error\n",
    "    error = RNN.backProp()\n",
    "    #once our error/loss is small enough\n",
    "    print(\"Error on iteration \", i, \": \", error)\n",
    "    if error > -100 and error < 100 or i % 100 == 0:\n",
    "        #we can finally define a seed word\n",
    "        seed = np.zeros_like(RNN.x)\n",
    "        maxI = np.argmax(np.random.random(RNN.x.shape))\n",
    "        seed[maxI] = 1\n",
    "        RNN.x = seed  \n",
    "        #and predict some new text!\n",
    "        output = RNN.sample()\n",
    "        print(output)    \n",
    "        #write it all to disk\n",
    "        ExportText(output, data)\n",
    "        print(\"Done Writing\")\n",
    "print(\"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
