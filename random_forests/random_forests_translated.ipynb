{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests - The Math of Intelligence (Week 7)\n",
    "\n",
    "\n",
    "## Demo\n",
    "\n",
    "![alt text](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/creditdecisiontree.png \"Logo Title Text 1\")\n",
    "\n",
    "우리는 랜덤 포레스트라는 기계 학습 모델에 대해 배우게 될 것입니다. 그 업무는 재무 기록을 사용하는 사람의 신용 위험을 평가하는 것입니다. 보험 회사에 유용합니다.\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\n",
    "\n",
    "이 데이터 집합은 일련의 속성으로 묘사된 사람들을 신용 위험이 좋거나 나쁜 것으로 분류합니다.\n",
    "\n",
    "\n",
    "## 랜덤 포레스트란?\n",
    "\n",
    "### 결정 트리 기반\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png \"Logo Title Text 1\")\n",
    "\n",
    "분류 및 회귀 트리 또는 간단히 CART 란 분류 또는 회귀 예측 모델링 문제에 사용할 수 있는 결정 트리 알고리즘을 언급하기 위해 Leo Breiman이 소개한 용어입니다.\n",
    "\n",
    "각 단계에서의 목표는 특정 목표 (즉, 원하는 출력 값)를 특정 변수의 특정 값과 연관시키는 것입니다. 결과는 각 경로가 특정 예측과 관련된 값의 조합을 식별하는 결정 트리입니다.\n",
    "\n",
    "이 트리에서 각각의 잎이 아닌 노드(내부노드)는 기본적으로 결정자입니다. 이러한 노드들을 결정 노드라고합니다. 각각의 노드는 다음 위치를 결정하는 특정 테스트를 수행합니다. 결과에 따라 당신은 이 노드의 왼쪽 분기 또는 오른쪽 분기로 이동합니다. 우리는 잎 노드(터미널노드)에 도달 할 때까지이 과정을 계속합니다. 분류자를 구성하는 경우 각 앞 노드는 클래스를 나타냅니다. 온도, 압력 및 바람과 같이 오늘 이용할 수있는 세 가지 요소를 기반으로 내일 비가 올지 여부를 결정하려고 한다고 해 봅시다. 따라서 전형적인 결정 트리는 다음과 같습니다:\n",
    "\n",
    "![alt text](https://prateekvjoshi.files.wordpress.com/2016/03/2-tree.png \"Logo Title Text 1\")\n",
    "\n",
    "그러나 어떻게 최적의 트리를 만들까요? 루트 노드에는 어떤 속성이 있어야합니까? 임계값은 어떻게 결정하나요? \n",
    "\n",
    "데이터 집합의 분할을 평가하는 데 사용되는 비용 함수로 지니 계수를 사용합니다. 우리는 그것을 최소화합니다.\n",
    "\n",
    "![alt text](http://i.imgur.com/IijgHbt.png \"Logo Title Text 1\")\n",
    "\n",
    "데이터 집합의 분할은 하나의 입력 속성과 그 속성에 대한 하나의 값을 포함합니다. 훈련 패턴을 두 개의 행 그룹으로 나누는 데 사용할 수 있습니다.\n",
    "\n",
    "Gini 점수는 분할로 인해 생성 된 두 그룹에 클래스가 얼마나 혼합되어 있는지에 따라 얼마나 좋은 분할인지에 대한 아이디어를 제공합니다. 완전한 분리는 Gini 점수가 0이되는 반면, 각 그룹의 50/50 클래스의 최악의 경우는 Gini 점수가 1.0이됩니다 (2 클래스 문제의 경우).\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-41-638.jpg?cb=1444928106 \"Logo Title Text 1\")\n",
    "\n",
    "우리는 모든 행에 대해 Gini 점수를 계산하고 이진 트리에서 그에 따라 데이터를 분할합니다. 이 과정을 재귀적으로 반복합니다.\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-14-638.jpg?cb=1444928106\n",
    " \"Logo Title Text 1\")\n",
    " \n",
    "결정 트리를 이용하여 하나의 랜덤 포레스트를 만들 수 있습니다.\n",
    " \n",
    "하나의 큰 (깊은) 단일 DT로 발생할 수있는 한 가지 문제점은 과적합 할 수 있다는 것입니다. DT가 사람이 눈 차트를 암기하는 방식으로 training set을 할 수 있습니다.\n",
    "\n",
    "RF의 핵심은 과적합을 방지하는 것입니다. 이 작업은 특징들의 임의의 하위 집합들을 만들고 그 하위 집합들을 사용하여 더 작은 (얕은) 트리를 만든 후 그 하위 트리들을 결합하여 수행합니다.\n",
    "\n",
    "RF의 단점은 단일 프로세스를 가지고 있지만 병렬화가 가능하다면 느려질 수 있다는 것입니다.\n",
    "\n",
    "### 과반수 투표\n",
    "![alt text](https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg \"Logo Title Text 1\")\n",
    "\n",
    "### 데이터의 하위 집합\n",
    "![alt text](https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/sparkmlrandomforest.png \"Logo Title Text 1\")\n",
    "\n",
    "### 노드들은 특징 분할을 나타냅니다.\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-b17755d2e0ffb326d8c39b7f3e07e03b-c \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "## 다른 좋은 예들?\n",
    "\n",
    "우리는 주가를 회귀 문제 (시계열) 또는 분류 문제로 다룰수 있습니다.\n",
    "\n",
    "#### 주가 분류 (2 가지 예)\n",
    "\n",
    "https://github.com/qiulx026/stock-predictor-with-machine-learning-methods\n",
    "\n",
    "https://github.com/joy13/RandomForest\n",
    "\n",
    "Ignore the forest part for a moment, even a single tree can do regression. Each leaf holds a prediction value, which no longer is a class for regression. Given an input feature vector, you simply walk the tree as you'd do for a classification problem, and the resulting value in the leaf node is the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Algorithm\n",
    "#This module implements pseudo-random number generators for various distributions.\n",
    "#seeding the generated number makes our results reproducible (good for debugging)\n",
    "from random import seed\n",
    "#Return a randomly selected element from range(start, stop, step). \n",
    "from random import randrange\n",
    "#read CSV file (dataset)\n",
    "from csv import reader\n",
    "#square root function\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    #init the dataset as a list\n",
    "\tdataset = list()\n",
    "    #open it as a readable file\n",
    "\twith open(filename, 'r') as file:\n",
    "        #init the csv reader\n",
    "\t\tcsv_reader = reader(file)\n",
    "        #for every row in the dataset\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "            #add that row as an element in our dataset list (2D Matrix of values)\n",
    "\t\t\tdataset.append(row)\n",
    "    #return in-memory data matrix\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    #iterate throw all the rows in our data matrix\n",
    "\tfor row in dataset:\n",
    "        #for the given column index, convert all values in that column to floats\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    #store a given column \n",
    "    class_values = [row[column] for row in dataset]\n",
    "    #create an unordered collection with no duplicates, only unique valeus\n",
    "    unique = set(class_values)\n",
    "    #init a lookup table\n",
    "    lookup = dict()\n",
    "    #for each element in the column\n",
    "    for i, value in enumerate(unique):\n",
    "        #add it to our lookup table\n",
    "        lookup[value] = i\n",
    "    #the lookup table stores pointers to the strings\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    #return the lookup table\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "# the original sample is randomly partitioned into k equal sized subsamples. \n",
    "#Of the k subsamples, a single subsample is retained as the validation data \n",
    "#for testing the model, and the remaining k − 1 subsamples are used as training data. \n",
    "#The cross-validation process is then repeated k times (the folds),\n",
    "#with each of the k subsamples used exactly once as the validation data.\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    #init 2 empty lists for storing split dataubsets\n",
    "\tleft, right = list(), list()\n",
    "    #for every row\n",
    "\tfor row in dataset:\n",
    "        #if the value at that row is less than the given value\n",
    "\t\tif row[index] < value:\n",
    "            #add it to list 1\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "            #else add it list 2 \n",
    "\t\t\tright.append(row)\n",
    "    #return both lists\n",
    "\treturn left, right\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    #how many correct predictions?\n",
    "\tcorrect = 0\n",
    "    #for each actual label\n",
    "\tfor i in range(len(actual)):\n",
    "        #if actual matches predicted label\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "            #add 1 to the correct iterator\n",
    "\t\t\tcorrect += 1\n",
    "    #return percentage of predictions that were correct\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    #folds are the subsamples used to train and validate model\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "    #for each subsample\n",
    "\tfor fold in folds:\n",
    "        #create a copy of the data\n",
    "\t\ttrain_set = list(folds)\n",
    "        #remove the given subsample\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "        #init a test set\n",
    "\t\ttest_set = list()\n",
    "        #add each row in a given subsample to the test set\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "        #get predicted labls\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "        #get actual labels\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "        #compare accuracy\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "        #add it to scores list, for each fold\n",
    "\t\tscores.append(accuracy)\n",
    "    #return all accuracy scores\n",
    "\treturn scores\n",
    " \n",
    " \n",
    "# Calculate the Gini index for a split dataset\n",
    "## this is the name of the cost function used to evaluate splits in the dataset.\n",
    "# this is a measure of how often a randomly chosen element from the set \n",
    "#would be incorrectly labeled if it was randomly labeled according to the distribution\n",
    "#of labels in the subset. Can be computed by summing the probability\n",
    "#of an item with label i being chosen times the probability \n",
    "#of a mistake in categorizing that item. \n",
    "#It reaches its minimum (zero) when all cases in the node \n",
    "#fall into a single target category.\n",
    "#A split in the dataset involves one input attribute and one value for that attribute. \n",
    "#It can be used to divide training patterns into two groups of rows.\n",
    "#A Gini score gives an idea of how good a split is by how mixed the classes \n",
    "#are in the two groups created by the split. A perfect separation results in \n",
    "#a Gini score of 0, whereas the worst case split that results in 50/50 classes \n",
    "#in each group results in a Gini score of 1.0 (for a 2 class problem).\n",
    "#We first need to calculate the proportion of classes in each group.\n",
    "def gini_index(groups, class_values):\n",
    "\tgini = 0.0\n",
    "    #for each class\n",
    "\tfor class_value in class_values:\n",
    "        #a random subset of that class\n",
    "\t\tfor group in groups:\n",
    "\t\t\tsize = len(group)\n",
    "\t\t\tif size == 0:\n",
    "\t\t\t\tcontinue\n",
    "            #average of all class values\n",
    "\t\t\tproportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "            #  sum all (p * 1-p) values, this is gini index\n",
    "\t\t\tgini += (proportion * (1.0 - proportion))\n",
    "\treturn gini\n",
    " \n",
    "# Select the best split point for a dataset\n",
    "#This is an exhaustive and greedy algorithm\n",
    "def get_split(dataset, n_features):\n",
    "    ##Given a dataset, we must check every value on each attribute as a candidate split, \n",
    "    #evaluate the cost of the split and find the best possible split we could make.\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "            ##When selecting the best split and using it as a new node for the tree \n",
    "            #we will store the index of the chosen attribute, the value of that attribute \n",
    "            #by which to split and the two groups of data split by the chosen split point.\n",
    "            ##Each group of data is its own small dataset of just those rows assigned to the \n",
    "            #left or right group by the splitting process. You can imagine how we might split \n",
    "            #each group again, recursively as we build out our decision tree.\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    ##Once the best split is found, we can use it as a node in our decision tree.\n",
    "    ##We will use a dictionary to represent a node in the decision tree as \n",
    "    #we can store data by name. \n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    " \n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    #select a class value for a group of rows. \n",
    "\toutcomes = [row[-1] for row in group]\n",
    "    #returns the most common output value in a list of rows.\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "#Create child splits for a node or make terminal\n",
    "#Building a decision tree involves calling the above developed get_split() function over \n",
    "#and over again on the groups created for each node.\n",
    "#New nodes added to an existing node are called child nodes. \n",
    "#A node may have zero children (a terminal node), one child (one side makes a prediction directly) \n",
    "#or two child nodes. We will refer to the child nodes as left and right in the dictionary representation \n",
    "#of a given node.\n",
    "#Once a node is created, we can create child nodes recursively on each group of data from \n",
    "#the split by calling the same function again.\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    #Firstly, the two groups of data split by the node are extracted for use and \n",
    "    #deleted from the node. As we work on these groups the node no longer requires access to these data.\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "    \n",
    "    #Next, we check if either left or right group of rows is empty and if so we create \n",
    "    #a terminal node using what records we do have.\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "    #We then check if we have reached our maximum depth and if so we create a terminal node.\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "    #We then process the left child, creating a terminal node if the group of rows is too small, \n",
    "    #otherwise creating and adding the left node in a depth first fashion until the bottom of \n",
    "    #the tree is reached on this branch.\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "\t# process right child\n",
    "    #The right side is then processed in the same manner, \n",
    "    #as we rise back up the constructed tree to the root.\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
    " \n",
    "#Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    #Building the tree involves creating the root node and \n",
    "\troot = get_split(train, n_features)\n",
    "    #calling the split() function that then calls itself recursively to build out the whole tree.\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    " \n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    #Making predictions with a decision tree involves navigating the \n",
    "    #tree with the specifically provided row of data.\n",
    "    #Again, we can implement this using a recursive function, where the same prediction routine is \n",
    "    #called again with the left or the right child nodes, depending on how the split affects the provided data.\n",
    "    #We must check if a child node is either a terminal value to be returned as the prediction\n",
    "    #, or if it is a dictionary node containing another level of the tree to be considered.\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    " \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [48.78048780487805, 60.97560975609756, 63.41463414634146, 58.536585365853654, 43.90243902439025]\n",
      "Mean Accuracy: 55.122%\n",
      "Trees: 5\n",
      "Scores: [53.65853658536586, 58.536585365853654, 60.97560975609756, 70.73170731707317, 63.41463414634146]\n",
      "Mean Accuracy: 61.463%\n",
      "Trees: 10\n",
      "Scores: [73.17073170731707, 63.41463414634146, 60.97560975609756, 53.65853658536586, 58.536585365853654]\n",
      "Mean Accuracy: 61.951%\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with a list of bagged trees\n",
    "#responsible for making a prediction with each decision tree and \n",
    "#combining the predictions into a single return value. \n",
    "#This is achieved by selecting the most common prediction \n",
    "#from the list of predictions made by the bagged trees.\n",
    "def bagging_predict(trees, row):\n",
    "\tpredictions = [predict(tree, row) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    " \n",
    "# Random Forest Algorithm\n",
    "#esponsible for creating the samples of the training dataset, training a decision tree on each,\n",
    "#then making predictions on the test dataset using the list of bagged trees.\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test the random forest algorithm\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "\tscores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "\tprint('Trees: %d' % n_trees)\n",
    "\tprint('Scores: %s' % scores)\n",
    "\tprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
