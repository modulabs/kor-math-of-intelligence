{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN, 순환 신경망)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy로 만든 RNN을 이용해 단어 생성을 수행한다.\n",
    "\n",
    "<img src=\"http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<br>우린 주어진 문단에서 다음 단어를 어떻게 예측하도록 네트워크를 학습시키는지 볼것이다. 이 작업은 네트워크가 단어들의 시퀀스를 기억하고 난 뒤의 순환 아키텍쳐가 요구된다. 순서도 중요하다. 1000번 반복하면 발음 가능한 것을 얻을 수 있다. 트레이닝 시간을 길게 할수록 (더) 좋다. 어떤 문장 배열이든 넣을 수 있다. (단어들, 파이썬, HTML, 기타 등등)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>순환 네트워크(Recurrent Network)란 무엇인가?\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피드포워드 네트워크(Feedforward network)는 입력과 출력간의 패턴을 배우는데 좋다.<br><br>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png\"><br>\n",
    "\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg\"><br>\n",
    "\n",
    "- 기온 & 위치\n",
    "- 높이 & 무게\n",
    "- 차의 속도와 브랜드<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 데이터의 순서가 중요하다면 어떨까?<br><br>\n",
    "\n",
    "<img src=\"http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg\" width=\"300\" height=\"200\"><br><br>\n",
    "\n",
    "<img src=\"http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2\" width=\"500\" height=\"500\"><br><br>\n",
    "\n",
    "\n",
    "알파벳, 노래의 가사. 이것들은 조건부 기억(Conditional Memory)를 사용해 저장된다.  만약 이전 요소에 접근했다면, 오로지 하나의 요소에만 접근할 수 있다. (linked list - 자료구조 형태처럼)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순환 네트워크로 들어가보자.<br><br>\n",
    "\n",
    "이전 시간의 단계에서 다음 시간 단계의 네트워크로 돌아가 hidden state를 집어넣는다.<br>\n",
    "\n",
    "<img src=\"https://iamtrask.github.io/img/basic_recurrence_singleton.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 이런식으로 데이터의 실행이 일어나는 대신<br>\n",
    "\n",
    "#### input -> hidden -> output<br><br>\n",
    "\n",
    "이렇게 나타난다.<br>\n",
    "\n",
    "#### (input + prev_hidden) -> hidden -> output<br><br>\n",
    "\n",
    "잠깐, 왜 이런 형태가 아니지?<br>\n",
    "\n",
    "#### (input + prev_input) -> hidden -> output<br><br>\n",
    " \n",
    "배선된 입력 순환(input recurrence)은 즉시 이전의 데이터 포인트만 기억할 뿐인데도, 숨겨진 순환(hidden recurrence)은 무엇을 기억해야할지 배운다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN : Intuition**<br><br>\n",
    "- RNN은 80년대 시계열 모델링을 위해 나온 신경망 모델이다.<br>\n",
    "- 네트워크의 구조는 피드포워드 네트워크(Feedforward network)와 비슷하며, 이전 시간에 의존해 각 시간마다 활성화되는 순환 hidden state를 사용한다는 점이 (다른 신경망과) 구별되는 부분이다.<br><br>\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311\" width=\"500\" height=\"300\"><br>\n",
    "\n",
    "<img src=\"https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png\" width=\"500\" height=\"300\"><br><br>\n",
    "\n",
    "**RNN 공식**<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png\" width=\"400\" height=\"200\"><br>\n",
    "기본적으로 현재 hidden state *h(t)*는 이전 hidden state *h(t-1)*와 현재 입력값 *x(t)*의 함수 *f* 라고 말할 수 있다. theta들은 함수 *f* 의 파라미터들이다. 네트워크는 일반적으로 *h(t)*를 사용해서 배우며, 일종의 *t* 의 입력값들의 과거 시퀀스 양상과 관련된 손실 요약본이라 볼 수 있다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>**손실 함수**<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5.\" width=\"500\" height=\"300\"><br><br>\n",
    "\n",
    "주어진 x값 시퀀스와 쌍인 y값의 시퀀스의 총 손실은, 모든 시간 단계별 손실의 합이다. 예시로, 만약 L(t)가 x(1)~x(t)가 주어졌을 때 y(t)의 negative log-likelihood라면, 그것들을 전부 더해서 시퀀스의 손실을 구할 수 있다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단계<br>\n",
    "\n",
    "- 가중치를 랜덤하게 초기화한다<br>\n",
    "- 문자쌍을 모델에 넣는다 (입력 문자 & 목표 문자. 목표 문자는 네트워크가 추측해야 하는 시퀀스 내부의 다음 문자이다.)<br>\n",
    "- 에러 측정 (이전 확률과 목표 문자 사이의 거리)<br>\n",
    "- 손실에 각 파라미터가 끼치는 영향을 보기 위한 기울기 계산 (시간 관통 역전파)<br>\n",
    "- 손실을 최소화하는 기울기를 통하는 방향으로 전체 파라미터 업데이트<br>\n",
    "- 반복! 손실이 확실하게 small이 될때까지<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 경우에 쓰이나?<br>\n",
    "- 시계열 예측 (날씨 예보, 주가, 교통량 기타 등등)<br>\n",
    "- 연속 데이터 생성 (음악, 비디오, 음향 기타 등등)<br><br>\n",
    "\n",
    "다른 예시들<br>\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)<br><br>\n",
    "\n",
    "다음은?<br>\n",
    "1 LSTM (Long Short-Term Memory) 네트워크<br>\n",
    "2 양방향 네트워크 (Bidirectional networks)<br>\n",
    "3 재귀 네트워크 (Recursive networks)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드의 네 부분<br>\n",
    "------------------------\n",
    "* 트레이닝 데이터 로딩<br>\n",
    " - 문자를 벡터로 인코딩<br>\n",
    "* Recurrent Network (순환 네트워크) 정의<br>\n",
    "* 손실 함수(loss function) 정의<br>\n",
    " - 포워드 과정<br>\n",
    " - 손실<br>\n",
    " - 백워드 과정<br>\n",
    "* 모델로부터 문장을 만들기 위한 함수 정의<br>\n",
    "* 네트워크 트레이닝<br>\n",
    " - 네트워크에 입력<br>\n",
    " - 기울기 계산과 모델 파라미터 업데이트<br>\n",
    " - 트레이닝의 향상성을 보기 위한 텍스트 출력<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레이닝 데이터 로딩<br>\n",
    "-----------------\n",
    "\n",
    "네트워크는 입력값으로 큰 텍스트 파일이 필요하다. 파일에 담긴 콘텐츠는 네트워크를 트레이닝 시키는데에 사용될 것이다. 카프카의 변신을 사용했다(저작권에서 자유롭다) 카프카는 이상한 친구라서 좋으니까 ;) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코딩/디코딩 문자/벡터<br>\n",
    "-----------------------\n",
    "\n",
    "신경망은 벡터로 동작하므로 (벡터는 float의 array다) 글자를 벡터로 encode / decode 할 방법이 필요하다.<br>고유한 글자들의 수를 셀건데 (vocabulary size) 이것이 벡터의 크기가 된다. 벡터는 문자의 위치(1이 있는)만 제외한 전부 0이다.<br><br>\n",
    "첫번째로 *vocab size*를 계산해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, '3': 1, '9': 2, ':': 3, 'M': 4, 'I': 5, '@': 6, '(': 7, 'K': 8, '$': 9, 's': 10, 'J': 11, 'f': 12, 'a': 13, '.': 14, '\"': 15, ' ': 16, ';': 17, 'Y': 18, 'l': 19, 'j': 20, 'C': 21, 'Q': 22, 'D': 23, 'S': 24, '\\n': 25, 'L': 26, 'U': 27, 'N': 28, 'P': 29, '7': 30, 'R': 31, 'u': 32, 'c': 33, 'G': 34, 'x': 35, 'F': 36, '!': 37, '5': 38, '%': 39, 'i': 40, 'H': 41, 'r': 42, 'd': 43, '8': 44, 'X': 45, 'b': 46, 'w': 47, 'n': 48, 't': 49, 'h': 50, 'v': 51, \"'\": 52, 'p': 53, 'm': 54, '1': 55, '2': 56, '4': 57, 'V': 58, ')': 59, 'o': 60, '*': 61, 'E': 62, 'z': 63, '챌': 64, 'O': 65, 'g': 66, '?': 67, 'T': 68, '0': 69, 'q': 70, 'e': 71, '/': 72, 'A': 73, 'k': 74, 'B': 75, 'W': 76, '6': 77, ',': 78, 'y': 79}\n",
      "{0: '-', 1: '3', 2: '9', 3: ':', 4: 'M', 5: 'I', 6: '@', 7: '(', 8: 'K', 9: '$', 10: 's', 11: 'J', 12: 'f', 13: 'a', 14: '.', 15: '\"', 16: ' ', 17: ';', 18: 'Y', 19: 'l', 20: 'j', 21: 'C', 22: 'Q', 23: 'D', 24: 'S', 25: '\\n', 26: 'L', 27: 'U', 28: 'N', 29: 'P', 30: '7', 31: 'R', 32: 'u', 33: 'c', 34: 'G', 35: 'x', 36: 'F', 37: '!', 38: '5', 39: '%', 40: 'i', 41: 'H', 42: 'r', 43: 'd', 44: '8', 45: 'X', 46: 'b', 47: 'w', 48: 'n', 49: 't', 50: 'h', 51: 'v', 52: \"'\", 53: 'p', 54: 'm', 55: '1', 56: '2', 57: '4', 58: 'V', 59: ')', 60: 'o', 61: '*', 62: 'E', 63: 'z', 64: '챌', 65: 'O', 66: 'g', 67: '?', 68: 'T', 69: '0', 70: 'q', 71: 'e', 72: '/', 73: 'A', 74: 'k', 75: 'B', 76: 'W', 77: '6', 78: ',', 79: 'y'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>다음으로 문자를 숫자로 인코딩 디코딩할 딕셔너리 2개를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 이렇게 문자들로부터 벡터를 만든다: <br>\n",
    "위에 정의된 딕셔너리는 256 대신 61 사이즈의 벡터를 만들 수 있게 해준다. <br>\n",
    "여기 'a' 문자 예시를 보자. 벡터에는 char_to_ix['a']에 1이 들어가는 위치 외에는 0만 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크 정의<br>\n",
    "------------------\n",
    "\n",
    "신경망은 3개의 레이어로 구성된다.<br>\n",
    "- 입력 레이어<br>\n",
    "- 히든 레이어<br>\n",
    "- 출력 레이어<br><br>\n",
    "\n",
    "모든 레이어는 다음으로 완전 연결(fully connected)이 된다 : 한 레이어의 각 노드는 다음 레이어의 전체 노드로 연결된다.<br>\n",
    "모든 히든 레이어는 아웃풋과 그 자신으로 연결되어 있다 : 반복(iteration)으로부터 얻어진 값은 다음 반복에 사용된다.<br> \n",
    "\n",
    "값을 중앙화 하는 건 트레이닝에서 중요하다(*hyper parameters*) *시퀀스의 길이(sequence length)* 와 *학습률(learning rate)* 도 마찬가지로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 파라미터들은 트레이닝을 하는 동안 맞춰진다.<br><br>\n",
    "- *WxH* 는 벡터와 관련된 파라미터들이다. 하나의 입력값에서 히든 레이어 까지 담고 있다.<br>\n",
    "- *Whh* 는 히든 레이어, 그 자신과 연결된 파라미터들이다. RNN의 키(Key)이다 : hidden state의 이전 값들로부터, 다음 반복(iteration)에 자기자신으로 다시 들어가는 것으로 순환이 된다.<br>\n",
    "- *Why*는 히든 레이어에서 출력으로 연결된 파라미터들이다.<br><br>\n",
    "- *bh*는 히든 바이어스 (hidden bias)를 갖고 있다.<br>\n",
    "- *by*는 출력 바이어스 (output bias)를 갖고 있다.<br><br>\n",
    "\n",
    "다음 섹션에서 이 파라미터들이 문장을 만드는데 어떻게 사용되는지 보자.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 함수 정의<br>\n",
    "---------------------------------------------\n",
    "\n",
    "모든 신경망을 트레이닝시키는 것이 손실(Loss)의 핵심 컨셉이다. 얼마나 우리의 모델이 좋은지 알려준다. 손실을 줄일수록, 더 나은 모델이다. (좋은 모델은 트레이닝 출력값에 근접한 예측 값을 내놓는 모델이다.) 트레이닝을 하는 동안 우리는 손실을 줄이고 싶다.<br><br>\n",
    "\n",
    "손실 함수는 손실과 함께 기울기를 계산한다 (백워드 과정을 봐라)<br>\n",
    "* 포워드 과정을 수행한다 : 트레이닝 세트로부터 주어진 문자의 다음 문자를 계산한다<br>\n",
    "* 예측한 문자와 목표 문자를 비교해서 손실을 계산한다 (트레이닝 셋에서 목표 문자는 이어지는 문자의 입력이다)<br>\n",
    "* 기울기 연산을 위해 백워드 과정을 계산한다<br><br>\n",
    "\n",
    "이 함수가 받는 입력: <br>\n",
    "* 입력 글자 리스트<br>\n",
    "* 목표 글자 리스트<br>\n",
    "* 이전 hidden state<br><br>\n",
    "\n",
    "이 함수의 출력들:<br>\n",
    "* 손실<br>\n",
    "* 레이어 사이의 각 파라미터를 위한 기울기<br>\n",
    "* 마지막 hidden state<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "포워드 과정<br>\n",
    "----------------\n",
    "\n",
    "포워드 과정(foward pass)은 트레이닝 세트로부터 주어진 문자의 다음 문자 계산을 위해, 모델의 파라미터(Wxh, Whh, Why, bh, by)를 사용한다<br><br>\n",
    "\n",
    "xs[t]는 t 위치에 있는 문자 인코딩 벡터<br>\n",
    "ps[t]는 다음 문자의 확률들<br><br>\n",
    "\n",
    "<img src=\"https://deeplearning4j.org/img/recurrent_equation.png\" width=\"400\" height=\"300\"><br><br>\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "<br><br><br>\n",
    "또는 각 문자별 의사 코드<br><br>\n",
    "```python\n",
    "hs = input * Wxh + last_value_of_hidden_state * Whh + bh\n",
    "ys = hs * Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "백워드 과정(Backward pass)<br>\n",
    "-------------------\n",
    "\n",
    "모든 기울기를 계산하는 단순한 방법은 각 파라미터별 작은 변화의 손실을 재계산하는 것이며, 할 수는 있지만 시간을 잡아먹는다. 모든 파라미터별 모든 기울기를 한 번에 계산하는 테크닉이 있다 : backdrop 전파이다. 기울기를 포워드 과정(forward pass)의 반대 방향으로 계산하는, 간단한 테크닉을 쓴다.<br><br>\n",
    "\n",
    "목표는 forward 공식을 위한 기울기 계산<br><br>\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh<br>\n",
    "ys = hs*Why + by\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "한 데이터 포인트에서의 손실<br>\n",
    "<img src=\"http://i.imgur.com/LlIMvek.png\" width=\"400\" height=\"300\"><br>\n",
    "\n",
    "손실을 줄여나가는 f(함수) 내부의 스코어 변화를 어떻게 계산할 수 있을까? 이를 위해 기울기(gradient)가 필요하다.<br><br>\n",
    "\n",
    "모든 출력 유닛들이 각 히든 유닛 에러에 영향을 주므로, 우리는 시퀀스에서 각 시간 단계별 계산된 기울기를 모두 더하고 파라미터를 업데이트하는데에 사용한다. 따라서 파라미터 기울기는 :<br><br><br>\n",
    "\n",
    "<img src=\"http://i.imgur.com/Ig9WGqP.png\" width=\"400\" height=\"300\"><br><br>\n",
    "\n",
    "손실의 첫번째 기울기. 연쇄법칙(chain rule)을 통해 역전파(backpropagate)해보자.<br> \n",
    "<img src=\"http://i.imgur.com/SOJcNLg.png\" width=\"400\" height=\"300\"><br><br>\n",
    "\n",
    "연쇄법칙은 합성 함수(composite function)의 도함수(derivative)를 찾아내는 방법이다. 또는 하나 또는 그 이상의 함수들의 결합으로 함수들을 만든다.<br>\n",
    "<img src=\"http://i.imgur.com/3Z2Rfdi.png\" width=\"400\" height=\"300\"><br><br>\n",
    "<img src=\"http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg\" width=\"400\" height=\"300\"><br><br>\n",
    "<img src=\"https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900\" width=\"400\" height=\"300\"><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass                                                                                                                                                                              \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "        \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # output probabilities\n",
    "        dy = np.copy(ps[t])\n",
    "        # derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y  \n",
    "        # compute output gradient -  output times hidden states transpose\n",
    "        # When we apply the transpose weight matrix,  \n",
    "        # we can think intuitively of this as moving the error backward\n",
    "        # through the network, giving us some sort of measure of the error \n",
    "        # at the output of the lth layer. \n",
    "        # output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # derivative of output bias\n",
    "        dby += dy\n",
    "        # backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw # derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) # derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) # derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델로부터 문장 생성\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " )d3FwQ65\n",
      ";;C97j3u2ISzcIVBjEUpR\n",
      "\n",
      "Qi*adlv7t-jyfPPdm poHJ2)GrYoT챌J\"챌eva''.6w\"aG9HU%2,0A,:Vr(W,)UX'o8iw,-R Q1.. \"eoHgdMea2mv73JN\n",
      "s)J8i$P.xp(gMJV$Uxb,wehJQ,PUAVj,I8kS2,X3I9Qk3TmUMk,\"D$9LXn3E%1p0hbq8Y,D2Tqp \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    # customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    # list to store generated chars\n",
    "    ixes = []\n",
    "    # for as many characters as we want to generate\n",
    "    for t in range(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "# predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트레이닝<br>\n",
    "-------------\n",
    "\n",
    "코드의 마지막 부분은 트레이닝 loop의 메인이다 : <br>\n",
    "* 네트워크에 파일의 일부를 넣는다. 파일 일부의 사이즈는 *seq_length*다.<br>\n",
    "* 사용하는 손실 함수는:<br>\n",
    "- 주어진 입력/출력 쌍을 위한 모델의 모든 파라미터들을 forward pass 에서 계산한다<br>\n",
    "- 모든 기울기를 backward pass에서 계산한다.<br>\n",
    "* 네트워크의 파라미터들을 사용해, 랜덤한 seed로부터 문장을 출력한다.<br>\n",
    "* Adagrad (Adaptive Gradient) 테크닉을 사용해 모델을 업데이트한다.<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**입력값과 목표값을 손실함수에 넣기**<br>\n",
    "\n",
    "데이터 파일로부터 문자 어레이를 두개 만들고, 타겟 어레이는 움직이며 입력 어레이와 비교한다.\n",
    "입력 어레이의 각 문자마다, 타겟 어레이는 이어지는 문자를 내놓는다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [65, 48, 71, 16, 54, 60, 42, 48, 40, 48, 66, 78, 16, 47, 50, 71, 48, 16, 34, 42, 71, 66, 60, 42, 16]\n",
      "targets [48, 71, 16, 54, 60, 42, 48, 40, 48, 66, 78, 16, 47, 50, 71, 48, 16, 34, 42, 71, 66, 60, 42, 16, 24]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**파라미터 업데이트를 위한 Adagrad**<br><br>\n",
    "\n",
    "경사 하강법(gradient descent)의 일종이다.\n",
    "\n",
    "<img src=\"http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\"><br>\n",
    "\n",
    "\n",
    "스텝 사이즈 = 학습률(learning rate)<br>\n",
    "\n",
    "모델의 파라미터를 업데이트하는 가장 쉬운 테크닉은 이거다:<br>\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "Adagrad는 트레이닝동안 step_size를 줄여나가는 더 효율적인 테크닉이다<br><br>\n",
    "시간마다 증가하는 메모리 변수를 사용하고:<br>\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "step_size를 계산하기 위해 이를 사용한다<br>\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "<br><br>\n",
    "\n",
    "요약: <br>\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smooth_loss**<br>\n",
    "\n",
    "smooth_loss는 트레이닝에서 어떤 일도 하지 않는다. 손실의 low pass filtered version이다. <br>\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "<br>\n",
    "\n",
    "마지막 반복의 손실 평균을 구하는 방법이 진척도를 추적하기 더 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로<br>\n",
    "---------------\n",
    "\n",
    "아래는 트레이닝과 시간마다 텍스트 생성을 함께 수행하는 main loop 코드이다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550658\n",
      "----\n",
      " O%nPLacKsv)YF-%bng.G1c9-@5$2Vu\n",
      "p4DX(*5ndR1 Y.de saOP7Y1) 'IRpMTVf\n",
      "lY RE(ekw%E;7\n",
      "uyT0q7F?*챌aO,Kb(PRs0qc1\"C@dJTUyzhyMsyHKwv.QuAaV)\"BQ;K:@KoO챌Gb(QL34R@q3.;E0!vk2sVLGnr?H' SFig1w.K,Sl)h)lYs!01dMgbJeE7y(/t \n",
      "----\n",
      "iter 1000, loss: 41.878356\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 2000, loss: 15.419662\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 3000, loss: 5.681937\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 4000, loss: 2.097729\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 5000, loss: 0.777827\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregmr Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 6000, loss: 0.291247\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 7000, loss: 0.111479\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 8000, loss: 0.044761\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 9000, loss: 0.019762\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 10000, loss: 0.010205\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 11000, loss: 0.006397\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 12000, loss: 0.004756\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 13000, loss: 0.003951\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 14000, loss: 0.003483\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 15000, loss: 0.003164\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 16000, loss: 0.002918\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 17000, loss: 0.002716\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 18000, loss: 0.002543\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 19000, loss: 0.002392\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 20000, loss: 0.002258\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 21000, loss: 0.002138\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 22000, loss: 0.002031\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 23000, loss: 0.001933\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 24000, loss: 0.001845\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 25000, loss: 0.001764\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 26000, loss: 0.001690\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 27000, loss: 0.001622\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 28000, loss: 0.001559\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 29000, loss: 0.001501\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 30000, loss: 0.001447\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 31000, loss: 0.001397\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 32000, loss: 0.001350\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 33000, loss: 0.001306\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 34000, loss: 0.001265\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 0.001226\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 36000, loss: 0.001190\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 37000, loss: 0.001155\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 38000, loss: 0.001123\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 39000, loss: 0.001092\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 40000, loss: 0.001063\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 41000, loss: 0.001036\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 42000, loss: 0.001009\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 43000, loss: 0.000985\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 44000, loss: 0.000961\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 45000, loss: 0.000938\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 46000, loss: 0.000917\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 47000, loss: 0.000896\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 48000, loss: 0.000876\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 49000, loss: 0.000857\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 50000, loss: 0.000839\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 51000, loss: 0.000822\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 52000, loss: 0.000805\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 53000, loss: 0.000789\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 54000, loss: 0.000774\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 55000, loss: 0.000759\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 56000, loss: 0.000745\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 57000, loss: 0.000731\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 58000, loss: 0.000718\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 59000, loss: 0.000705\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 60000, loss: 0.000693\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 61000, loss: 0.000681\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 62000, loss: 0.000669\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 63000, loss: 0.000658\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 64000, loss: 0.000647\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 65000, loss: 0.000637\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 66000, loss: 0.000627\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 67000, loss: 0.000617\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 68000, loss: 0.000607\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 69000, loss: 0.000598\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 70000, loss: 0.000589\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 71000, loss: 0.000581\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 72000, loss: 0.000572\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 73000, loss: 0.000564\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 74000, loss: 0.000556\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 75000, loss: 0.000548\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 76000, loss: 0.000541\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 77000, loss: 0.000533\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 78000, loss: 0.000526\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 79000, loss: 0.000519\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 80000, loss: 0.000512\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 81000, loss: 0.000506\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 82000, loss: 0.000499\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 83000, loss: 0.000493\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 84000, loss: 0.000487\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 85000, loss: 0.000481\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 86000, loss: 0.000475\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 87000, loss: 0.000469\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 88000, loss: 0.000464\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 89000, loss: 0.000458\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 90000, loss: 0.000453\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 91000, loss: 0.000448\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 92000, loss: 0.000443\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 93000, loss: 0.000438\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 94000, loss: 0.000433\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 95000, loss: 0.000428\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 96000, loss: 0.000424\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 97000, loss: 0.000419\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 98000, loss: 0.000415\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 99000, loss: 0.000410\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n",
      "iter 100000, loss: 0.000406\n",
      "----\n",
      " ne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor Sne morning, when Gregor S \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
