{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network \n",
    "\n",
    "## Demo\n",
    "\n",
    "우리는 사용자가 그린 어떤 알페벳을 인식하기 할 수 있는 numpy 를 사용한 CNN 을 만들 것이다. 그것은 모두 당신이 브라우저에서 재생 할 수 있도록 Flask 웹어플리케이션으로 감쌀것이다.\n",
    "\n",
    "![alt text](http://i.imgur.com/8ysCoB5.png \"Logo Title Text 1\")\n",
    "\n",
    "## What inspired Convolutional Networks?\n",
    "\n",
    "CNN 은 D. H. Hubel 과 T. N. Wiesel 의 연구를 통해 생물학적으로 영감을 얻은 모델이다. \n",
    "그들은 포유 동물이 뇌의 신경층 구조를 사용하여 주위의 세계를 시각적으로 인식하는 방법에 대한 설명을 제안했으며 이는 엔지니어가 컴퓨터 비전에서도 유사한 패턴 인식 메커니즘을 개발 하도록 유도했다.\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-235acb60a481423eaf70c39b17bc914b.webp \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "그들의 가설에서, 시각 피질 내에서 \"복합 세포\"에 의해 생성 된 복잡한 기능적 반응은 \"단순 세포\"에서 보다 단순한 반응으로 구성 된다. \n",
    "\n",
    "예를 들어, 단순한 세포는 지향성 가장자리 등에 반응하지만 복잡한 세포는 지향성 가장자리에도 반응하지만 어느 정도의 공간 불변선에 반응한다.\n",
    "\n",
    "수용 영역은 셀에 존재하며, 셀은 다른 로컬 셀의 입력 합계에 응답한다.\n",
    "\n",
    "DCNN 의 아키텍처는 위에서 언급한 아이디어에서 영감을 받았다.\n",
    "\n",
    "- 로컬 연결\n",
    "- 레이어링\n",
    "- 공간 불변량(입력 신호를 이동 시키면 출력 신호가 똑같이 이동한다.) 우리는 추상을 배우기 때문에 다양한 조건에서 특정면을 인식 할 수 있다. 따라서 이러한 추상화는 크기, 대비, 회전, 방향에 영향을 주지 않는다.\n",
    " \n",
    "그러나, 이러한 CNN 의 계산 메커니즘이 primate 시각 시스템에서 발생하는 계산 메커니즘과 유사한 지 여보는 여전히 남아있다.\n",
    "\n",
    "- convolution operation\n",
    "- shared weights\n",
    "- pooling/subsampling \n",
    "\n",
    "## How does it work? \n",
    "\n",
    "![alt text](https://images.nature.com/w926/nature-assets/srep/2016/160610/srep27755/images_hires/srep27755-f1.jpg \"Logo Title Text 1\")\n",
    "![alt text](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1497876372993.jpg \"Logo Title Text 1\")\n",
    "\n",
    "### Step 1 - Prepare a dataset of images\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png \"Logo Title Text 1\")\n",
    "\n",
    "- 모든 이미지는 픽셀 값의 행렬이다. \n",
    "- 각 픽셀에서 인코딩 할 수 있는 값의 범위는 비트 크기에 따라 다르다.  \n",
    "- 가장 일반적으로, 8비트 또는 1바이트 크기의 픽셀이 있다. 따라서 한 픽셀이 표현 할 수 있는 값의 범우는 [0, 255] 이다. \n",
    "- 그러나 색상이미지, 특히 RGB(빨강, 초록, 파랑) 기반 이미지의 경우 별도의 색상 채널(RGB 이미지의 경우 3채널)이 있어 데이터에 'depth' 필드를 추가하여 입력을 3차원으로 만든다.\n",
    "- 주어진 RGB 이미지에 대해 각 이미지와 연관된 3개의 행렬, 각 색상 채널에 대한 행렬이 있다. 이런 까닭에 주어진 255×255 (가로 x 세로) 픽셀 크기의 RGB 이미지는 각 색상 채널당 한개씩 이미지와 연관된 3개의 행렬를 가진다.\n",
    "- 따라서 이미지는 전체적으로 입력 볼륨 (255x255x3) 이라는 3차원 구조를 구성한다.\n",
    "\n",
    "Great training datasets are [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) and [CoCo](http://mscoco.org/). We'll use CIFAR.\n",
    "\n",
    "### Step 2 - Convolution \n",
    "\n",
    "![alt text](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/more_images/Convolution_schematic.gif \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_2.png \"Logo Title Text 1\")\n",
    "\n",
    "- convolution 은 두가지 정보 소스가 서로 얽혀있는 순차적인 절차이다.\n",
    "\n",
    "- 커널(필터) 는 이미지의 입력 크기와 비교하여 작은 크기의 행렬이며 실수 값의 항목으로 구성된다.\n",
    "\n",
    "- 커널은 입력 볼륨과 컨볼류션되어 '엑티베이션맵' (또는 피처맵이라고도 함)을 얻는다.\n",
    "\n",
    "- 엑티베이션 맵은 활성화 된 영역, 즉 커널에서 특정 기능이 입력에서 감지 된 영역을 나타낸다.\n",
    "\n",
    "- 커널 행력의 실제 값은 학습 데이터에 대한 학습 반복 마다 변경되어 네트워크가 데이터에서 피쳐를 추출하는데 중요한 영역을 식별하는 것을 학습하고 있음을 나타낸다.\n",
    "\n",
    "- 우리는 커널과 입력 행렬 사이의 내적을 계산한다\n",
    "- 내적으로부터 결과적인 항들을 합산함으로써 얻어지는 컨볼루션 된 값은 엑티베이션 행렬에서 단일 엔트리를 형성한다.\n",
    "\n",
    "- 패치 선택은 'stride' 값이라는 특정 양만큼 슬라이딩(오른쪽으로 또는 매트릭스의 경계에 도달하면 아래쪽으로)되며 전체 입력 이미지가 처리 될 때까지 프로세스가 반복된다. - 이 프로세스는 모든 색상 채널에 대해 수행 된다.\n",
    "\n",
    "- 모든 뉴런을 모든 가능한 픽셀에 연결하는 대신 입력의 전체 깊이까지 확장되는 '리셉티브 필드[14]' (5X5 단위 크기) 의 2차원 영역을 지정한다. (3색 채널의 경우 5X5X3 입력), 포함 된 픽셀이 뉴럴 네트워크의 입력 레이어에 완전히 연결된다. 네트워크 영역 횐단면(('depth columns'라고 불리는) 여러 뉴런으로 구성된 이 엑티베이션 맵을 조작하고 생성하는 것이 이 작은 영역을 넘는다.(계산 복잡도 감소)\n",
    "\n",
    "![alt text](http://i.imgur.com/g4hRI6Z.png \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/tpQvMps.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://i.imgur.com/oyXkhHi.jpg \"Logo Title Text 1\")\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_5.png \"Logo Title Text 1\")\n",
    "\n",
    "Great resource on description of  convolution (discrete vs continous)  & the fourier transform\n",
    "컨볼루션(이산 vs 연속) 및 푸리에 변환에 대한 설명 좋은 리소스\n",
    "\n",
    "http://timdettmers.com/2015/03/26/convolution-deep-learning/\n",
    "\n",
    "\n",
    "###  Step 3 - Pooling\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure_6.png \"Logo Title Text 1\")\n",
    "\n",
    "- 풀링은 다음 컨볼루션 레이어를 위해 입력 볼륨의 차원(가로 X 세로)을 줄인다.\n",
    "- 변환은 윈도우 안에서 관찰 할 수 있는 값으로 부터 최대 값을 취하거나('맥스 풀링'이라 부르는), 값의 평균을 취하여 수행된다.\n",
    "- 또한 다운샘플링이라 불린다.\n",
    "\n",
    "###  Step 4 - Normalization (ReLU in our case)\n",
    "\n",
    "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/CodeCogsEqn-3.png \"Logo Title Text 1\")\n",
    "\n",
    "정규화(모든 음수를 0으로 설정하여 수학을 깨뜨리지 않도록 유지)(RELU) 이미지 스택은 음수 값이 없는 이미지 스택이된다. \n",
    "\n",
    "2-4 단계를 여러 번 반복해라. 더 작은 이미지(모든 레이어에서 만들어진 피처 맵)\n",
    "\n",
    "### Step 5 - Regularization \n",
    "\n",
    "- 드랍아웃은 인공 신경망이 학습 단계에서 뉴런을 무효로함으로써 동일한 데이터의 여러 독립적 인 표현을 학습하도록 강제한다.\n",
    "- 드랍아웃의 거의 모든 최첨단 뉴럴네트워크 구현에서 중요한 기능이다.\n",
    "- 레이어에서 드랍아웃을 수행하려면 전달 중에 레이어 값 중 일부를 임의로 0으로 설정한다.\n",
    "\n",
    "See [this](http://iamtrask.github.io/2015/07/28/dropout/)\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/CewjH.png \"Logo Title Text 1\")\n",
    "\n",
    "###  Step 6 - Probability Conversion\n",
    "\n",
    "네트워크 끝 부분에서는 softmax 함수를 적용하여 출력을 각 클래스의 확률 값으로 변환한다.\n",
    "\n",
    "![alt text](https://1.bp.blogspot.com/-FHDU505euic/Vs1iJjXHG0I/AAAAAAABVKg/x4g0FHuz7_A/s1600/softmax.JPG \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "###  Step 7 - 가장 가능성있는 레이블 선택 (max probability value) \n",
    "\n",
    "argmax(softmax_outputs)\n",
    "\n",
    "일곱번째 단계는 네트워크를 통한 순방향 전달이다.\n",
    "\n",
    "## 그렇다면 어떻게 magic number를 학습 할 수 있을까? \n",
    "\n",
    "- backpropagation 을 통해 특징과 가중치를 학습 할 수 있다.\n",
    "\n",
    "![alt text](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/images/cover.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/cnn-toupload-final-151117124948-lva1-app6892/95/convolutional-neural-networks-cnn-52-638.jpg?cb=1455889178 \"Logo Title Text 1\")\n",
    "\n",
    "다른 하이퍼 파라미터는 인간에 의해 설정되며 능동적인 연구 분야이다(최적의 하이퍼 파라미터를 찾는 것)\n",
    "\n",
    "i.e - 뉴런수, 피처 수, 피처 크기, 풀링 윈도우 크기, 윈도우 스트라이드\n",
    "\n",
    "\n",
    "## 언제 사용하는 것이 좋은가?\n",
    "\n",
    "- 이미지 분류를 위해\n",
    "- 이미지 생성을 위해 (나중에 더 많이..)\n",
    "\n",
    "![alt text](https://nlml.github.io/images/convnet_diagram.png \"Logo Title Text 1\")\n",
    "\n",
    "그러나 어떤 공간 2차원 또는 3차원 데이터에도 적용 할 수 있다. 이미지 소리와 텍스트조차도 경험적으로 보면 고객 데이터와 같은 행과 열을 바꿔서 CNN을 사용 할 수 없다면 데이터도 유용 할 것이다.\n",
    "\n",
    "\n",
    "## 좋은 예제\n",
    "\n",
    "Robot learns to grasp (combining CNNs)\n",
    "\n",
    "![alt text](https://img.newatlas.com/youtube-robot-6.jpg?auto=format%2Ccompress&fit=max&h=670&q=60&w=1000&s=d003e42afa7e462fd711c6a99f21b51f \"Logo Title Text 1\")\n",
    "\n",
    "Tensorflow! https://github.com/upul/CarND-TensorFlow-Lab\n",
    "\n",
    "Adversarial CNNs https://github.com/michbad/adversarial-mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-7-6ef6ea6446b0>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6ef6ea6446b0>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    self.vocab = meta[\"vocab\"]\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import pickle #saving and loading our serialized model \n",
    "import numpy as np #matrix math\n",
    "from app.model.preprocessor import Preprocessor as img_prep #image preprocessing\n",
    "\n",
    "#class for loading our saved model and classifying new images\n",
    "class LiteOCR:\n",
    "    \n",
    "\tdef __init__(self, fn=\"alpha_weights.pkl\", pool_size=2):\n",
    "        #load the weights from the pickle file and the meta data\n",
    "\t\t[weights, meta] = pickle.load(open(fn, 'rb'), encoding='latin1') #currently, this class MUST be initialized from a pickle file\n",
    "\t\t#list to store labels\n",
    "        self.vocab = meta[\"vocab\"]\n",
    "        \n",
    "        #how many rows and columns in an image\n",
    "\t\tself.img_rows = meta[\"img_side\"] ; self.img_cols = meta[\"img_side\"]\n",
    "        \n",
    "        #load our CNN\n",
    "\t\tself.CNN = LiteCNN()\n",
    "        #with our saved weights\n",
    "\t\tself.CNN.load_weights(weights)\n",
    "        #define the pooling layers size\n",
    "\t\tself.CNN.pool_size=int(pool_size)\n",
    "    \n",
    "    #classify new image\n",
    "\tdef predict(self, image):\n",
    "\t\tprint(image.shape)\n",
    "        #vectorize the image into the right shape for our network\n",
    "\t\tX = np.reshape(image, (1, 1, self.img_rows, self.img_cols))\n",
    "\t\tX = X.astype(\"float32\")\n",
    "        \n",
    "        #make the prediction\n",
    "\t\tpredicted_i = self.CNN.predict(X)\n",
    "        #return the predicted label\n",
    "\t\treturn self.vocab[predicted_i]\n",
    "\n",
    "class LiteCNN:\n",
    "\tdef __init__(self):\n",
    "        # a place to store the layers\n",
    "\t\tself.layers = [] \n",
    "        # size of pooling area for max pooling\n",
    "\t\tself.pool_size = None \n",
    "\n",
    "\tdef load_weights(self, weights):\n",
    "\t\tassert not self.layers, \"Weights can only be loaded once!\"\n",
    "        #add the saved matrix values to the convolutional network\n",
    "\t\tfor k in range(len(weights.keys())):\n",
    "\t\t\tself.layers.append(weights['layer_{}'.format(k)])\n",
    "\n",
    "\tdef predict(self, X):        \n",
    "        #here is where the network magic happens at a high level\n",
    "        h = self.cnn_layer(X, layer_i=0, border_mode=\"full\"); X= h\n",
    "        h = self.relu_layer(X); X = h;\n",
    "        h = self.cnn_layer(X, layer_i=2, border_mode=\"valid\"); X = h\n",
    "        h = self.relu_layer(X); X = h;\n",
    "        h = self.maxpooling_layer(X); X = h\n",
    "        h = self.dropout_layer(X, .25); X = h\n",
    "        h = self.flatten_layer(X, layer_i=7); X = h;\n",
    "        h = self.dense_layer(X, fully, layer_i=10); x = H\n",
    "        h = self.softmax_layer2D(X); x = h\n",
    "        max_i = self.classify(X)\n",
    "        return max_i[0]\n",
    "    \n",
    "    #given our feature map we've learned from convolving around the image\n",
    "    #lets make it more dense by performing pooling, specifically max pooling\n",
    "    #we'll select the max values from the image matrix and use that as our new feature map\n",
    "\tdef maxpooling_layer(self, convolved_features):\n",
    "        #given our learned features and images\n",
    "\t\tnb_features = convolved_features.shape[0]\n",
    "\t\tnb_images = convolved_features.shape[1]\n",
    "\t\tconv_dim = convolved_features.shape[2]\n",
    "\t\tres_dim = int(conv_dim / self.pool_size)       #assumed square shape\n",
    "\n",
    "        #initialize our more dense feature list as empty\n",
    "\t\tpooled_features = np.zeros((nb_features, nb_images, res_dim, res_dim))\n",
    "        #for each image\n",
    "\t\tfor image_i in range(nb_images):\n",
    "            #and each feature map\n",
    "\t\t\tfor feature_i in range(nb_features):\n",
    "                #begin by the row\n",
    "\t\t\t\tfor pool_row in range(res_dim):\n",
    "                    #define start and end points\n",
    "\t\t\t\t\trow_start = pool_row * self.pool_size\n",
    "\t\t\t\t\trow_end   = row_start + self.pool_size\n",
    "\n",
    "                    #for each column (so its a 2D iteration)\n",
    "\t\t\t\t\tfor pool_col in range(res_dim):\n",
    "                        #define start and end points\n",
    "\t\t\t\t\t\tcol_start = pool_col * self.pool_size\n",
    "\t\t\t\t\t\tcol_end   = col_start + self.pool_size\n",
    "                        \n",
    "                        #define a patch given our defined starting ending points\n",
    "\t\t\t\t\t\tpatch = convolved_features[feature_i, image_i, row_start : row_end,col_start : col_end]\n",
    "                        #then take the max value from that patch\n",
    "                        #store it. this is our new learned feature/filter\n",
    "\t\t\t\t\t\tpooled_features[feature_i, image_i, pool_row, pool_col] = np.max(patch)\n",
    "\t\treturn pooled_features\n",
    "\n",
    "    #convolution is the most important of the matrix operations here\n",
    "    #well define our input, lauyer number, and a border mode (explained below)\n",
    "\tdef cnn_layer(self, X, layer_i=0, border_mode = \"full\"):\n",
    "        #we'll store our feature maps and bias value in these 2 vars\n",
    "\t\tfeatures = self.layers[layer_i][\"param_0\"]\n",
    "\t\tbias = self.layers[layer_i][\"param_1\"]\n",
    "        #how big is our filter/patch?\n",
    "\t\tpatch_dim = features[0].shape[-1]\n",
    "        #how many features do we have?\n",
    "\t\tnb_features = features.shape[0]\n",
    "        #How big is our image?\n",
    "\t\timage_dim = X.shape[2] #assume image square\n",
    "        #R G B values\n",
    "\t\timage_channels = X.shape[1]\n",
    "        #how many images do we have?\n",
    "\t\tnb_images = X.shape[0]\n",
    "        \n",
    "        #With border mode \"full\" you get an output that is the \"full\" size as the input. \n",
    "        #That means that the filter has to go outside the bounds of the input by \"filter size / 2\" - \n",
    "        #the area outside of the input is normally padded with zeros.\n",
    "\t\tif border_mode == \"full\":\n",
    "\t\t\tconv_dim = image_dim + patch_dim - 1\n",
    "        #With border mode \"valid\" you get an output that is smaller than the input because \n",
    "        #the convolution is only computed where the input and the filter fully overlap.\n",
    "\t\telif border_mode == \"valid\":\n",
    "\t\t\tconv_dim = image_dim - patch_dim + 1\n",
    "        \n",
    "        #we'll initialize our feature matrix\n",
    "\t\tconvolved_features = np.zeros((nb_images, nb_features, conv_dim, conv_dim));\n",
    "        #then we'll iterate through each image that we have\n",
    "\t\tfor image_i in range(nb_images):\n",
    "            #for each feature \n",
    "\t\t\tfor feature_i in range(nb_features):\n",
    "                #lets initialize a convolved image as empty\n",
    "\t\t\t\tconvolved_image = np.zeros((conv_dim, conv_dim))\n",
    "                #then for each channel (r g b )\n",
    "\t\t\t\tfor channel in range(image_channels):\n",
    "                    #lets extract a feature from our feature map\n",
    "\t\t\t\t\tfeature = features[feature_i, channel, :, :]\n",
    "                    #then define a channel specific part of our image\n",
    "\t\t\t\t\timage   = X[image_i, channel, :, :]\n",
    "                    #perform convolution on our image, using a given feature filter\n",
    "\t\t\t\t\tconvolved_image += self.convolve2d(image, feature, border_mode);\n",
    "\n",
    "                #add a bias to our convoved image\n",
    "\t\t\t\tconvolved_image = convolved_image + bias[feature_i]\n",
    "                #add it to our list of convolved features (learnings)\n",
    "\t\t\t\tconvolved_features[image_i, feature_i, :, :] = convolved_image\n",
    "\t\treturn convolved_features\n",
    "\n",
    "    #In a dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "\tdef dense_layer(self, X, layer_i=0):\n",
    "        #so we'll initialize our weight and bias for this layer\n",
    "\t\tW = self.layers[layer_i][\"param_0\"]\n",
    "\t\tb = self.layers[layer_i][\"param_1\"]\n",
    "        #and multiply it by our input (dot product)\n",
    "\t\toutput = np.dot(X, W) + b\n",
    "\t\treturn output\n",
    "\n",
    "\t@staticmethod\n",
    "    \n",
    "    #so what does the convolution operation look like?, given an image and a feature map (filter)\n",
    "\tdef convolve2d(image, feature, border_mode=\"full\"):\n",
    "        #we'll define the tensor dimensions of the image and the feature\n",
    "\t\timage_dim = np.array(image.shape)\n",
    "\t\tfeature_dim = np.array(feature.shape)\n",
    "        #as well as a target dimension\n",
    "\t\ttarget_dim = image_dim + feature_dim - 1\n",
    "        #then we'll perform a fast fourier transform on both the input and the filter\n",
    "        #performing a convolution can be written as a for loop but for many convolutions\n",
    "        #this approach is too comp. expensive/slow. it can be performed orders of magnitude\n",
    "        #faster using a fast fourier transform. \n",
    "\t\tfft_result = np.fft.fft2(image, target_dim) * np.fft.fft2(feature, target_dim)\n",
    "        #and set the result to our target \n",
    "\t\ttarget = np.fft.ifft2(fft_result).real\n",
    "\n",
    "\t\tif border_mode == \"valid\":\n",
    "\t\t\t# To compute a valid shape, either np.all(x_shape >= y_shape) or\n",
    "\t\t\t# np.all(y_shape >= x_shape).\n",
    "            #decide a target dimension to convolve around\n",
    "\t\t\tvalid_dim = image_dim - feature_dim + 1\n",
    "\t\t\tif np.any(valid_dim < 1):\n",
    "\t\t\t\tvalid_dim = feature_dim - image_dim + 1\n",
    "\t\t\tstart_i = (target_dim - valid_dim) // 2\n",
    "\t\t\tend_i = start_i + valid_dim\n",
    "\t\t\ttarget = target[start_i[0]:end_i[0], start_i[1]:end_i[1]]\n",
    "\t\treturn target\n",
    "\n",
    "\tdef relu_layer(x):\n",
    "        #turn all negative values in a matrix into zeros\n",
    "\t\tz = np.zeros_like(x)\n",
    "\t\treturn np.where(x>z,x,z)\n",
    "\n",
    "\tdef softmax_layer2D(w):\n",
    "        #this function will calculate the probabilities of each\n",
    "        #target class over all possible target classes. \n",
    "\t\tmaxes = np.amax(w, axis=1)\n",
    "\t\tmaxes = maxes.reshape(maxes.shape[0], 1)\n",
    "\t\te = np.exp(w - maxes)\n",
    "\t\tdist = e / np.sum(e, axis=1, keepdims=True)\n",
    "\t\treturn dist\n",
    "\n",
    "    #affect the probability a node will be turned off by multiplying it\n",
    "    #by a p values (.25 we define)\n",
    "\tdef dropout_layer(X, p):\n",
    "\t\tretain_prob = 1. - p\n",
    "\t\tX *= retain_prob\n",
    "\t\treturn X\n",
    "\n",
    "    #get the largest probabililty value from the list\n",
    "\tdef classify(X):\n",
    "\t\treturn X.argmax(axis=-1)\n",
    "\n",
    "    #tensor transformation, less dimensions\n",
    "\tdef flatten_layer(X):\n",
    "\t\tflatX = np.zeros((X.shape[0],np.prod(X.shape[1:])))\n",
    "\t\tfor i in range(X.shape[0]):\n",
    "\t\t\tflatX[i,:] = X[i].flatten(order='C')\n",
    "\t\treturn flatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
