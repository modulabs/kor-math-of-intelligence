{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression을 위한 Newton's Method\n",
    "- The Math of Intelligence (Week 2)\n",
    "\n",
    "\n",
    "![alt text](https://plot.ly/~florianh/140/logistic-regression-1-feature.png \"Logo Title Text 1\")\n",
    "\n",
    "## 목표\n",
    "\n",
    "우리는 어떤사람의 키, 몸무게, 혈압이 주어졌을 때 당뇨병에 걸렸을 확률을 계산하려고한다. 우리는 데이터(Toy data)를 만들고, 그래프로 그리고, 최적화를 위해 Newton's Method를 사용하여 logistic regression 곡선을 배울것이다. 그리고 이 커브를 이용하여 새로운 어떤사람의 키, 몸무게, 혈압이 주어졌을 때 그 사람이 당뇨병에 걸렸을 확률을 예측할 것이다. 우리는 미적분, 확률이론, 통계학, 선형대수를 사용할 것이다. 준비가 되었으며 아래로 넘어가도록 하자.\n",
    "\n",
    "\n",
    "## Logistic regression이란 무엇인가?\n",
    "\n",
    "Logistic regression에서 핵심으로 사용되는 함수인 logistic function의 이름을 따서 붙여졌다. linear regression에서는 그 결과(변수와 관련된)는 연속적이다. 이는 무한한 수의 가능한 수 중 하나를 가질 수 있다. logistic regression에서는 그 결과(변수와 관련된)로 제한된 수의 가능한 수를 가진다. Logistic regression은 응답변수가 완전히 분류될 때 사용된다.\n",
    "\n",
    "logistic 함수는 sigmoid 함수라고도 불린디ㅏ. 이 함수는 S형 곡선이며, 어떤 실수를 취하든 0과 1차이의 값의 값을 갖는다. 그러나 결코 그 한계에는 달하지 않는다.\n",
    "\n",
    "![alt text](https://qph.ec.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba \"Logo Title Text 1\")\n",
    "\n",
    "자연로그의 밑인 e(오일러 수 또는 너의 스프레드 시트에 있는 EXP() 함수) 그리고 그 값은 실제 계산해야할 수치라는 것에서, E는 매우 수학적으로 편리한 수이다. 예를 들어 너가 e^x를 미분할 때, 너는 e^x를 다시 얻을 수 있다. 이런 것을 할 수 있는 것은 지구상에 서 유일하다. \n",
    "\n",
    "logistic regression은 Linear regression과 비슷한 방정식을 표현을 위해 사용한다. Logistic regression의 주전제는 너의 입력은 직선의 경계로 각각의 Class는 두개의 좋은 영역으로 나눠진다고 가정하는 것이다. 너의 데이터는 n차원에서 선형적으로 분리가능 해야한다.\n",
    "\n",
    "![alt text](https://codesachin.files.wordpress.com/2015/08/linearly_separable_4.png \"Logo Title Text 1\")\n",
    "\n",
    "아래 함수를 보면\n",
    "\n",
    "![alt text](https://s0.wp.com/latex.php?latex=%5Cbeta_0+%2B+%5Cbeta_1+x_1+%2B+%5Cbeta_2+x_2&bg=ffffff&fg=555555&s=0&zoom=2 \"Logo Title Text 1\")\n",
    "\n",
    "주어진 점(a,b), 만약 너가 이것을 함수에 넣으면 위 방정식은 결과값으로 양수(하나의 class에 속함), 음수(다른 class에 속함) 또는 0(decision boundary 바로 위에 있는 경우)을 가질 수 있다.\n",
    "\n",
    "그래서 너는 입력값에 대해 -무한대에서 무한대의 결과값을 가질 수 있다. 그러나 우리는 어떻게 그것을 0과 1사이의 값을 갖는 확률로 Mapping 할까? 그것에 대한 해답은 Odds 함수에 있다.\n",
    "\n",
    "![alt text](http://i.imgur.com/bz8XqI8.png \"Logo Title Text 1\")\n",
    "\n",
    "$P(X)$ X가 일어날 확률로 보자 이경우 Odds 비(OR(X))는 아래와 같이 정의될 수 있다.\n",
    "\n",
    "![alt text](https://s0.wp.com/latex.php?latex=%5Cfrac%7BP%28X%29%7D%7B1-P%28X%29%7D&bg=ffffff&fg=555555&s=0&zoom=2 \"Logo Title Text 1\")\n",
    "\n",
    "이것은 사건이 일어날 확률과 확률이 일어나지 않을 확률의 비율이다. 확률과 Odds는 같은 정보를 전달하고 있다는 것은 명확하다. 그러나 $P(X)$는 0에서 1의 값을 가지고 OR(X)는 0에서 무한대의 값을 가진다.\n",
    "\n",
    "입력 (x)는 결과값 (y)를 예측하기 위해 weight또는 계수(그리스 문자로 표현된)와 선형적으로 결합되어 있다. linear regression과 가장 큰 차이는 그 결과값이 수치(연속적인)가 아닌 0 또는 1의 이진수(이산적인)라는 것이다.\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ihcclogisticregression-130728061733-phpapp02/95/logistic-regression-in-casecontrol-study-14-638.jpg?cb=1374992365 \"Logo Title Text 1\")\n",
    "\n",
    "그러나, 우리의 경계함수가 -무한대에서 무한대의 값을 갖기 때문에 좀 더 살펴볼 필요가 있다. 그래서 우리가 하는 것은, log-odds 함수라고 불리는, OR(X)에 로그를 취하는 것이다.\n",
    "수학적으로 OR(X)는 0에서 무한대의 값을 가진다, log(OR(X))는 -무한대에서 무한대의 값을 가진다.\n",
    "\n",
    "우리는 X를 (Y=1)이라는 기본 분류에 속할 확률을 모델링한다. 그 예측 확률을 실제 예측확률로 하기 위해 이진수(0또는 1)로 바뀌어야 한다. Logistic regression은 선형적 방법이다. 그러나 그 예측은 logistic 함수를 통해 이뤄진다. logistic 함수를 취하는 것은 우리가 더이상 linear regression에서 했던 것처럼 선형 결합으로 이해할 수 없다는 것이다.\n",
    "\n",
    "그럼 어떻게 boundary function을 계산할까? 우리는 임의의 데이터가 올바르게 구별될 우도를 최대화하는 것을 원한다. 이를 우리는 Maximimum likelihood estimation라고 한다.\n",
    "\n",
    "MLE는 통계적 모델에서 우도함수를 최대화하는 변수를 계상하는 일반적인 접근방법이다. MLE는 Deep 네트웨크에서는 Backpropagation이라고 부른다. MLE는 아래와 같이 정의된다.\n",
    "\n",
    "L(θ|X)=f(X|θ)\n",
    "\n",
    "Newton's Method는 최적화 알고리즘이다. 너는 우도 함수를 포함한 많은 함수의 최대/최소를 찾을 수 있다. 너는 다른 방법을 사용하여 MLE를 계산할 수 있고 최적화 알고리즘은 그것들 중 하나이다.\n",
    "\n",
    "## 왜 최적화를 위해 Newton's Method를 사용하는 가?\n",
    "\n",
    "- Newton's method는 logistic regression log likelihood를 최대화할때 gradient descent 보다 빠르게 수렴한다.\n",
    "\n",
    "- 각각의 반복은 Hassian의 역행렬을 계산해야하기 때문에 gradient descent보다 비싸다\n",
    "\n",
    "- 데이터가 크지 않다면 Newton'method가 더 좋다.\n",
    "\n",
    "## logistic regression + newton's method의 좋은 예제는 어떤 것일까? \n",
    "\n",
    "1 https://github.com/hhl60492/newton_logistic/blob/master/main.py (Spam Classification)\n",
    "2 https://github.com/yangarbiter/logistic_regression_newton-cg (Click Through Rate Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#matrix math\n",
    "import numpy as np\n",
    "#data manipulation\n",
    "import pandas as pd\n",
    "#matrix data structure\n",
    "from patsy import dmatrices\n",
    "#for error logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"data_setup\"></a>\n",
    "## 설정\n",
    "\n",
    "### 변수 / 데이터 설정\n",
    "\n",
    "아래 셀에서는 데이터 생성, 알고리즘 설정, 그리고 우리가 시도하고 맞게할 모델이 어떤 것인지를 할 수 있게 하는 많은 변수와 설정이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputs probability between 0 and 1, used to help define our logistic regression curve\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://i.imgur.com/TfPVnME.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes the random numbers predictable\n",
    "#(pseudo-)random numbers work by starting with a number (the seed), \n",
    "#multiplying it by a large number, then taking modulo of that product. \n",
    "#The resulting number is then used as the seed to generate the next \"random\" number. \n",
    "#When you set the seed (every time), it does the same thing every time, giving you the same numbers.\n",
    "#good for reproducing results for debugging\n",
    "\n",
    "\n",
    "np.random.seed(0) # set the seed\n",
    "\n",
    "##Step 1 - Define model parameters (hyperparameters)\n",
    "\n",
    "## algorithm settings\n",
    "#the minimum threshold for the difference between the predicted output and the actual output\n",
    "#this tells our model when to stop learning, when our prediction capability is good enough\n",
    "tol=1e-8 # convergence tolerance\n",
    "\n",
    "lam = None # l2-regularization\n",
    "#how long to train for?\n",
    "max_iter = 20 # maximum allowed iterations\n",
    "\n",
    "## data creation settings\n",
    "#Covariance measures how two variables move together. \n",
    "#It measures whether the two move in the same direction (a positive covariance) \n",
    "#or in opposite directions (a negative covariance). \n",
    "r = 0.95 # covariance between x and z\n",
    "n = 1000 # number of observations (size of dataset to generate) \n",
    "sigma = 1 # variance of noise - how spread out is the data?\n",
    "\n",
    "## model settings\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 # true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 # variances of inputs\n",
    "\n",
    "## the model specification you want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sraval/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>v</th>\n",
       "      <th>np.exp(x)</th>\n",
       "      <th>I(v ** 2 + z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309888</td>\n",
       "      <td>0.988232</td>\n",
       "      <td>5.137890</td>\n",
       "      <td>3.705758</td>\n",
       "      <td>27.386150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.300988</td>\n",
       "      <td>-0.184444</td>\n",
       "      <td>38.764534</td>\n",
       "      <td>0.740087</td>\n",
       "      <td>1502.504681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.920497</td>\n",
       "      <td>-0.874842</td>\n",
       "      <td>11.798098</td>\n",
       "      <td>0.398321</td>\n",
       "      <td>138.320272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.174965</td>\n",
       "      <td>0.161452</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>1.191205</td>\n",
       "      <td>0.161453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.755467</td>\n",
       "      <td>0.778161</td>\n",
       "      <td>1.347385</td>\n",
       "      <td>2.128606</td>\n",
       "      <td>2.593607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.555271</td>\n",
       "      <td>0.844654</td>\n",
       "      <td>97.468804</td>\n",
       "      <td>1.742413</td>\n",
       "      <td>9501.012390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.735170</td>\n",
       "      <td>0.443148</td>\n",
       "      <td>-139.913964</td>\n",
       "      <td>2.085836</td>\n",
       "      <td>19576.360461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.168627</td>\n",
       "      <td>-0.156636</td>\n",
       "      <td>32.123394</td>\n",
       "      <td>0.844824</td>\n",
       "      <td>1031.755826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>-0.166830</td>\n",
       "      <td>-56.340269</td>\n",
       "      <td>1.268723</td>\n",
       "      <td>3174.059121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.522206</td>\n",
       "      <td>1.153393</td>\n",
       "      <td>71.220417</td>\n",
       "      <td>4.582321</td>\n",
       "      <td>5073.501239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.338652</td>\n",
       "      <td>-0.347083</td>\n",
       "      <td>102.569929</td>\n",
       "      <td>0.712730</td>\n",
       "      <td>10520.243283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.437052</td>\n",
       "      <td>-0.227711</td>\n",
       "      <td>-13.617018</td>\n",
       "      <td>0.645938</td>\n",
       "      <td>185.195455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.794423</td>\n",
       "      <td>-0.704078</td>\n",
       "      <td>-391.418061</td>\n",
       "      <td>0.451842</td>\n",
       "      <td>153207.394470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.853165</td>\n",
       "      <td>0.731489</td>\n",
       "      <td>-104.585247</td>\n",
       "      <td>2.347063</td>\n",
       "      <td>10938.805299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.510078</td>\n",
       "      <td>1.613199</td>\n",
       "      <td>-66.692121</td>\n",
       "      <td>4.527085</td>\n",
       "      <td>4449.452178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.584762</td>\n",
       "      <td>-0.627751</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.557238</td>\n",
       "      <td>-0.627748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.258404</td>\n",
       "      <td>1.221651</td>\n",
       "      <td>-1.956957</td>\n",
       "      <td>3.519800</td>\n",
       "      <td>5.051331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.462204</td>\n",
       "      <td>2.351425</td>\n",
       "      <td>11.471611</td>\n",
       "      <td>11.730636</td>\n",
       "      <td>133.949284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.028601</td>\n",
       "      <td>0.347158</td>\n",
       "      <td>384.361644</td>\n",
       "      <td>0.971805</td>\n",
       "      <td>147734.220458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.266787</td>\n",
       "      <td>-1.365889</td>\n",
       "      <td>-265.060904</td>\n",
       "      <td>0.281735</td>\n",
       "      <td>70255.916701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.495442</td>\n",
       "      <td>-0.319084</td>\n",
       "      <td>281.819388</td>\n",
       "      <td>0.609301</td>\n",
       "      <td>79421.848314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.160855</td>\n",
       "      <td>0.758244</td>\n",
       "      <td>0.684946</td>\n",
       "      <td>0.851415</td>\n",
       "      <td>1.227395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.332411</td>\n",
       "      <td>-0.097091</td>\n",
       "      <td>-15.476945</td>\n",
       "      <td>0.717193</td>\n",
       "      <td>239.438730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.425501</td>\n",
       "      <td>-0.693561</td>\n",
       "      <td>1.761897</td>\n",
       "      <td>0.653442</td>\n",
       "      <td>2.410719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.539670</td>\n",
       "      <td>-1.885771</td>\n",
       "      <td>37.932199</td>\n",
       "      <td>0.214452</td>\n",
       "      <td>1436.965981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.132232</td>\n",
       "      <td>-1.208478</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.322313</td>\n",
       "      <td>-1.208477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.171040</td>\n",
       "      <td>0.075793</td>\n",
       "      <td>111.370935</td>\n",
       "      <td>0.842788</td>\n",
       "      <td>12403.560919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.172880</td>\n",
       "      <td>-2.624163</td>\n",
       "      <td>8.536982</td>\n",
       "      <td>0.113849</td>\n",
       "      <td>70.255898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.166936</td>\n",
       "      <td>0.976308</td>\n",
       "      <td>0.079060</td>\n",
       "      <td>3.212137</td>\n",
       "      <td>0.982559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.410733</td>\n",
       "      <td>-0.567065</td>\n",
       "      <td>-0.009517</td>\n",
       "      <td>0.663164</td>\n",
       "      <td>-0.566974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.539835</td>\n",
       "      <td>-1.143572</td>\n",
       "      <td>-247.689819</td>\n",
       "      <td>0.214416</td>\n",
       "      <td>61349.102706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.632153</td>\n",
       "      <td>-1.492005</td>\n",
       "      <td>-13.226907</td>\n",
       "      <td>0.195508</td>\n",
       "      <td>173.459068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.764530</td>\n",
       "      <td>-1.628392</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.465553</td>\n",
       "      <td>-1.627773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.695984</td>\n",
       "      <td>0.122698</td>\n",
       "      <td>-23.304949</td>\n",
       "      <td>2.005683</td>\n",
       "      <td>543.243344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.977293</td>\n",
       "      <td>-0.954703</td>\n",
       "      <td>-2.203118</td>\n",
       "      <td>0.376329</td>\n",
       "      <td>3.899026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.034838</td>\n",
       "      <td>0.051344</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.965762</td>\n",
       "      <td>0.051766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.987639</td>\n",
       "      <td>-1.250847</td>\n",
       "      <td>4.915997</td>\n",
       "      <td>0.372455</td>\n",
       "      <td>22.916176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.369994</td>\n",
       "      <td>0.348958</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>1.447726</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.395449</td>\n",
       "      <td>-0.274797</td>\n",
       "      <td>0.148369</td>\n",
       "      <td>0.673377</td>\n",
       "      <td>-0.252783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.807571</td>\n",
       "      <td>-0.846933</td>\n",
       "      <td>20.337651</td>\n",
       "      <td>0.445940</td>\n",
       "      <td>412.773107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.138599</td>\n",
       "      <td>-341.225302</td>\n",
       "      <td>0.904566</td>\n",
       "      <td>116434.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.309325</td>\n",
       "      <td>0.539759</td>\n",
       "      <td>0.432925</td>\n",
       "      <td>1.362506</td>\n",
       "      <td>0.727183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.478389</td>\n",
       "      <td>-0.516518</td>\n",
       "      <td>-0.051512</td>\n",
       "      <td>0.619781</td>\n",
       "      <td>-0.513864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.344510</td>\n",
       "      <td>-0.330705</td>\n",
       "      <td>454.142390</td>\n",
       "      <td>0.708567</td>\n",
       "      <td>206244.979892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.980338</td>\n",
       "      <td>2.246921</td>\n",
       "      <td>6.981049</td>\n",
       "      <td>7.245195</td>\n",
       "      <td>50.981963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>-0.733672</td>\n",
       "      <td>744.695504</td>\n",
       "      <td>1.119804</td>\n",
       "      <td>554570.659717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.165867</td>\n",
       "      <td>-0.534900</td>\n",
       "      <td>-36.110413</td>\n",
       "      <td>0.847159</td>\n",
       "      <td>1303.427004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973260</td>\n",
       "      <td>1.262661</td>\n",
       "      <td>-154.601778</td>\n",
       "      <td>2.646558</td>\n",
       "      <td>23902.972511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.800440</td>\n",
       "      <td>-152.660552</td>\n",
       "      <td>2.047088</td>\n",
       "      <td>23306.044575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.461598</td>\n",
       "      <td>0.510788</td>\n",
       "      <td>-12.789236</td>\n",
       "      <td>1.586608</td>\n",
       "      <td>164.075341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.919909</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>-88.942402</td>\n",
       "      <td>2.509062</td>\n",
       "      <td>7911.474052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.754402</td>\n",
       "      <td>-1.052098</td>\n",
       "      <td>-3.346875</td>\n",
       "      <td>0.470292</td>\n",
       "      <td>10.149475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.667074</td>\n",
       "      <td>-0.762721</td>\n",
       "      <td>-1.236098</td>\n",
       "      <td>0.513208</td>\n",
       "      <td>0.765217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329666</td>\n",
       "      <td>0.052049</td>\n",
       "      <td>0.732793</td>\n",
       "      <td>1.390504</td>\n",
       "      <td>0.589034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.294987</td>\n",
       "      <td>-2.477418</td>\n",
       "      <td>-50.803419</td>\n",
       "      <td>0.100763</td>\n",
       "      <td>2578.509943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.577144</td>\n",
       "      <td>-1.593487</td>\n",
       "      <td>-8.868699</td>\n",
       "      <td>0.206564</td>\n",
       "      <td>77.060332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.598048</td>\n",
       "      <td>-0.603793</td>\n",
       "      <td>-157.242463</td>\n",
       "      <td>0.549884</td>\n",
       "      <td>24724.588319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.453198</td>\n",
       "      <td>-0.345774</td>\n",
       "      <td>-139.334162</td>\n",
       "      <td>0.635592</td>\n",
       "      <td>19413.662869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.116760</td>\n",
       "      <td>-0.269528</td>\n",
       "      <td>-0.213530</td>\n",
       "      <td>0.889799</td>\n",
       "      <td>-0.223933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.643911</td>\n",
       "      <td>-0.658863</td>\n",
       "      <td>1.694710</td>\n",
       "      <td>0.525234</td>\n",
       "      <td>2.213181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n",
       "0         1.0  1.309888  0.988232    5.137890   3.705758      27.386150\n",
       "1         1.0 -0.300988 -0.184444   38.764534   0.740087    1502.504681\n",
       "2         1.0 -0.920497 -0.874842   11.798098   0.398321     138.320272\n",
       "3         1.0  0.174965  0.161452    0.001030   1.191205       0.161453\n",
       "4         1.0  0.755467  0.778161    1.347385   2.128606       2.593607\n",
       "5         1.0  0.555271  0.844654   97.468804   1.742413    9501.012390\n",
       "6         1.0  0.735170  0.443148 -139.913964   2.085836   19576.360461\n",
       "7         1.0 -0.168627 -0.156636   32.123394   0.844824    1031.755826\n",
       "8         1.0  0.238011 -0.166830  -56.340269   1.268723    3174.059121\n",
       "9         1.0  1.522206  1.153393   71.220417   4.582321    5073.501239\n",
       "10        1.0 -0.338652 -0.347083  102.569929   0.712730   10520.243283\n",
       "11        1.0 -0.437052 -0.227711  -13.617018   0.645938     185.195455\n",
       "12        1.0 -0.794423 -0.704078 -391.418061   0.451842  153207.394470\n",
       "13        1.0  0.853165  0.731489 -104.585247   2.347063   10938.805299\n",
       "14        1.0  1.510078  1.613199  -66.692121   4.527085    4449.452178\n",
       "15        1.0 -0.584762 -0.627751   -0.001936   0.557238      -0.627748\n",
       "16        1.0  1.258404  1.221651   -1.956957   3.519800       5.051331\n",
       "17        1.0  2.462204  2.351425   11.471611  11.730636     133.949284\n",
       "18        1.0 -0.028601  0.347158  384.361644   0.971805  147734.220458\n",
       "19        1.0 -1.266787 -1.365889 -265.060904   0.281735   70255.916701\n",
       "20        1.0 -0.495442 -0.319084  281.819388   0.609301   79421.848314\n",
       "21        1.0 -0.160855  0.758244    0.684946   0.851415       1.227395\n",
       "22        1.0 -0.332411 -0.097091  -15.476945   0.717193     239.438730\n",
       "23        1.0 -0.425501 -0.693561    1.761897   0.653442       2.410719\n",
       "24        1.0 -1.539670 -1.885771   37.932199   0.214452    1436.965981\n",
       "25        1.0 -1.132232 -1.208478    0.001036   0.322313      -1.208477\n",
       "26        1.0 -0.171040  0.075793  111.370935   0.842788   12403.560919\n",
       "27        1.0 -2.172880 -2.624163    8.536982   0.113849      70.255898\n",
       "28        1.0  1.166936  0.976308    0.079060   3.212137       0.982559\n",
       "29        1.0 -0.410733 -0.567065   -0.009517   0.663164      -0.566974\n",
       "..        ...       ...       ...         ...        ...            ...\n",
       "70        1.0 -1.539835 -1.143572 -247.689819   0.214416   61349.102706\n",
       "71        1.0 -1.632153 -1.492005  -13.226907   0.195508     173.459068\n",
       "72        1.0 -0.764530 -1.628392    0.024868   0.465553      -1.627773\n",
       "73        1.0  0.695984  0.122698  -23.304949   2.005683     543.243344\n",
       "74        1.0 -0.977293 -0.954703   -2.203118   0.376329       3.899026\n",
       "75        1.0 -0.034838  0.051344    0.020535   0.965762       0.051766\n",
       "76        1.0 -0.987639 -1.250847    4.915997   0.372455      22.916176\n",
       "77        1.0  0.369994  0.348958   -0.000105   1.447726       0.348958\n",
       "78        1.0 -0.395449 -0.274797    0.148369   0.673377      -0.252783\n",
       "79        1.0 -0.807571 -0.846933   20.337651   0.445940     412.773107\n",
       "80        1.0 -0.100300 -0.138599 -341.225302   0.904566  116434.568185\n",
       "81        1.0  0.309325  0.539759    0.432925   1.362506       0.727183\n",
       "82        1.0 -0.478389 -0.516518   -0.051512   0.619781      -0.513864\n",
       "83        1.0 -0.344510 -0.330705  454.142390   0.708567  206244.979892\n",
       "84        1.0  1.980338  2.246921    6.981049   7.245195      50.981963\n",
       "85        1.0  0.113153 -0.733672  744.695504   1.119804  554570.659717\n",
       "86        1.0 -0.165867 -0.534900  -36.110413   0.847159    1303.427004\n",
       "87        1.0  0.973260  1.262661 -154.601778   2.646558   23902.972511\n",
       "88        1.0  0.716418  0.800440 -152.660552   2.047088   23306.044575\n",
       "89        1.0  0.461598  0.510788  -12.789236   1.586608     164.075341\n",
       "90        1.0  0.919909  0.723214  -88.942402   2.509062    7911.474052\n",
       "91        1.0 -0.754402 -1.052098   -3.346875   0.470292      10.149475\n",
       "92        1.0 -0.667074 -0.762721   -1.236098   0.513208       0.765217\n",
       "93        1.0  0.329666  0.052049    0.732793   1.390504       0.589034\n",
       "94        1.0 -2.294987 -2.477418  -50.803419   0.100763    2578.509943\n",
       "95        1.0 -1.577144 -1.593487   -8.868699   0.206564      77.060332\n",
       "96        1.0 -0.598048 -0.603793 -157.242463   0.549884   24724.588319\n",
       "97        1.0 -0.453198 -0.345774 -139.334162   0.635592   19413.662869\n",
       "98        1.0 -0.116760 -0.269528   -0.213530   0.889799      -0.223933\n",
       "99        1.0 -0.643911 -0.658863    1.694710   0.525234       2.213181\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2 - Generate and organize our data\n",
    "\n",
    "#The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal \n",
    "#distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix.\n",
    "#so we generate values input values - (x, v, z) using normal distributions\n",
    "\n",
    "#A probability distribution is a function that provides us the probabilities of all \n",
    "#possible outcomes of a stochastic process. \n",
    "\n",
    "#lets keep x and z closely related (height and weight)\n",
    "x, z = np.random.multivariate_normal([0,0], [[var_x,r],[r,var_z]], n).T\n",
    "#blood presure\n",
    "v = np.random.normal(0,var_v,n)**3\n",
    "\n",
    "#create a pandas dataframe (easily parseable object for manipulation)\n",
    "A = pd.DataFrame({'x' : x, 'z' : z, 'v' : v})\n",
    "#compute the log odds for our 3 independent variables\n",
    "#using the sigmoid function \n",
    "A['log_odds'] = sigmoid(A[['x','z','v']].dot([beta_x,beta_z,beta_v]) + sigma*np.random.normal(0,1,n))\n",
    "\n",
    "\n",
    "\n",
    "#compute the probability sample from binomial distribution\n",
    "#A binomial random variable is the number of successes x has in n repeated trials of a binomial experiment. \n",
    "#The probability distribution of a binomial random variable is called a binomial distribution. \n",
    "A['y'] = [np.random.binomial(1,p) for p in A.log_odds]\n",
    "\n",
    "#create a dataframe that encompasses our input data, model formula, and outputs\n",
    "y, X = dmatrices(formula, A, return_type='dataframe')\n",
    "\n",
    "#print it\n",
    "X.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"algorithms\"></a>\n",
    "<hr>\n",
    "### 알고리즘 설정\n",
    "\n",
    "우리는 Newton 단계를 표현하는데 사용할 특이 함수의 에러를 잡는 간단한 함수에서 시작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#like dividing by zero (Wtff omgggggg universe collapses)\n",
    "def catch_singularity(f):\n",
    "    '''Silences LinAlg Errors and throws a warning instead.'''\n",
    "    \n",
    "    def silencer(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian!')\n",
    "            return args[0]\n",
    "    return silencer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"newton\"></a>\n",
    "<hr>\n",
    "### Single Newton 단계에 대한 설명\n",
    "\n",
    "주어진 함수$f(\\beta)$의 최대화/최소화를 위한 Newton's Method를 보면, 아래와 같은 추산을 반복적으로 계산한다.\n",
    "\n",
    "$$\n",
    "\\beta^+ = \\beta - Hf(\\beta)^{-1}\\nabla f(\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "logistic regression를 위한 Hessian의 log-likelihood 아래와 같이 주어진다:\n",
    "\n",
    "hessian of our function = negative tranpose of (N times (p+1) times (N x N diagional matrix of weights, each is p*(1-p) times X again\n",
    "\n",
    "\n",
    "$$\n",
    "Hf(\\beta) = -X^TWX\n",
    "$$\n",
    "gradient는 아래와 같다\n",
    "\n",
    "우리 함수의 gradient = X의 Transpose X (열 벡터 - N 확률 벡터)\n",
    "\n",
    "$$\n",
    "\\nabla f(\\beta) = X^T(y-p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "W := \\text{diag}\\left(p(1-p)\\right)\n",
    "$$\n",
    "와 $p$가 현재 $\\beta$의 값에서 계산된 예상된 확률일 때.\n",
    "\n",
    "### Connection to Iteratively Reweighted Least Squares (IRLS)\n",
    "*For logistic regression, this step is actually equivalent to computing a weighted least squares estimator at each iteration!* \n",
    "The method of least squares is about estimating\n",
    "parameters by minimizing the squared discrepancies\n",
    "between observed data, on the one hand, and their\n",
    "expected values on the other\n",
    "\n",
    "I.e.,\n",
    "$$\n",
    "\\beta^+ = \\arg\\min_\\beta (z-X\\beta)^TW(z-X\\beta)\n",
    "$$\n",
    "with $W$ as before and the *adjusted response* $z$ is given by\n",
    "$$\n",
    "z := X\\beta + W^{-1}(y-p)\n",
    "$$\n",
    "\n",
    "**Takeaway:** This is fun, but in fact it can be leveraged to derive asymptotics and inferential statistics about the computed MLE $\\beta^*$!\n",
    "\n",
    "### 구현\n",
    "\n",
    "아래는 우리는 single step of Newton's method를 구현했다, 그리고 $Ax = b$를 풀기위해 우리는 $Hf(\\beta)^{-1}\\nabla f(\\beta)$를 `np.linalg.lstsq(A,b)`이용하여 계산한다. 이것은 실제 Hessian의 역행렬을 계산할 필요없다는 것을 알아두자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    #how to compute inverse? http://www.mathwarehouse.com/algebra/matrix/images/square-matrix/inverse-matrix.gif\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    #create probability matrix, miniminum 2 dimensions, tranpose (flip it)\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    #create weight matrix from it\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    #derive the hessian \n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    #derive the gradient\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization step (avoiding overfitting)\n",
    "    if lam:\n",
    "        # Return the least-squares solution to a linear matrix equation\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## update our \n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 우리는 Newton's method를 조금 다른 방법으로 구현한다.; 이 때 각각의 반복에서 우리는 전체 Hession의 역행렬을 `np.linalg.inv()`를 이용해 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        #Compute the inverse of a matrix.\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update our weights\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conv\"></a>\n",
    "<hr>\n",
    "### 수렴 설정\n",
    "\n",
    "처음 우리는 계수의 수렴을 구현한다; 이 정지 조건이 아래일 때 수렴이라고 한다.\n",
    "\n",
    "$$\n",
    "\\|\\beta^+ - \\beta\\|_\\infty < \\epsilon\n",
    "$$\n",
    " $\\epsilon$이 주어진 오차에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    #calculate the change in the coefficients\n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    #if change hasn't reached the threshold and we have more iterations to go, keep training\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"numerics\"></a>\n",
    "<hr>\n",
    "## 수학적 예제\n",
    "\n",
    "### 계수 수렴의 일반적인 Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 11\n",
      "Beta : [[  1.79279050e+17]\n",
      " [ -1.08284603e+18]\n",
      " [ -8.55860493e+17]\n",
      " [ -1.75734896e+18]\n",
      " [ -9.70028288e+17]\n",
      " [  5.18119068e+17]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sraval/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "#initial coefficients (weight values), 2 copies, we'll update one\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "#num iterations we've done so far\n",
    "iter_count = 0\n",
    "#have we reached convergence?\n",
    "coefs_converged = False\n",
    "\n",
    "#if we haven't reached convergence... (training step)\n",
    "while not coefs_converged:\n",
    "    \n",
    "    #set the old coefficients to our current\n",
    "    beta_old = beta\n",
    "    #perform a single step of newton's optimization on our data, set our updated beta values\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    #increment the number of iterations\n",
    "    iter_count += 1\n",
    "    \n",
    "    #check for convergence between our old and new beta values\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
